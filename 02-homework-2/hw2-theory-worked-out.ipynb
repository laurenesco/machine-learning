{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70169e01-ffa7-4ab4-8ec3-9c963de42195",
   "metadata": {},
   "source": [
    "# Problem 1 #\n",
    "\n",
    "<span style=\"color:lightgray\">\n",
    "[10 points] Consider running the Perceptron algorithm on a training set S arranged in a certain order. Now suppose we run it with the same initial weights and on the same training set but in a different order, S ′ . Does Perceptron make the same number of mistakes? Does it end up with the same final weights? If so, prove it. If not, give a counterexample, i.e. an S and S ′ where order matters.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9f05c4-753c-4613-932a-e2ad6c091bd5",
   "metadata": {},
   "source": [
    "## **Solution**\n",
    "\n",
    "I will provide a counterexample which proves that the order of S **will** affect the number of mistakes made, but the final weight vector remains the **same**.\n",
    "\n",
    "---\n",
    "\n",
    "## ----------------------- Begin Proof ----------------------- ##\n",
    "\n",
    "## **Training Set S<sub>1</sub>** ##\n",
    "Let $ S_1 = \\{([-1, 2],1),\\;([2, -1],-1),\\;([1, 1],1)\\} $ and $w_1 = [0,0]$\n",
    "\n",
    "### **Iteration 1** ### \n",
    "\n",
    "$$ f(x_1) = 0*(-1) + 0*2 = 0 $$ \n",
    "Prediction: 0 Label: 1. **Incorrect**, so update weight vector $w_1$: \n",
    "$$ w_{new} = [0, 0] + 1*[-1, 2] = [-1, 2] $$\n",
    "\n",
    "$$ f(x_2) = (-1)*2 + 2*(-1) = -4 $$\n",
    "Prediction: -1 Label: -1. Correct, so do not update weight vector. \n",
    "\n",
    "$$ f(x_3) = (-1)*1 + 2*1 = 1 $$\n",
    "Prediction: 1 Label: 1. Correct, so do not update weight vector. \n",
    "\n",
    "### **Iteration 2** ### \n",
    "\n",
    "$$ f(x_1) = (-1)*(-1) + 2*2 = 5 $$ \n",
    "Prediction: 1 Label: 1. Correct, so do not update weight vector. \n",
    "\n",
    "$$ f(x_2) = (-1)*2 + 2*(-1) = -4 $$\n",
    "Prediction: -1 Label: -1. Correct, so do not update weight vector. \n",
    "\n",
    "$$ f(x_3) = (-1)*1 + 2*1 = 1 $$\n",
    "Prediction: 1 Label: 1. Correct, so do not update weight vector. \n",
    "\n",
    "### Convergence reached. Mistakes: 1. w*: [-1, 2] ###\n",
    "\n",
    "---\n",
    "\n",
    "## **Training Set S<sub>2</sub>** ##\n",
    "Let $ S_1 = \\{([1, 1],1),\\;([2, -1],-1),\\;([-1, 2],1)\\} $ and $w_1 = [0,0]$\n",
    "\n",
    "### **Iteration 1** ### \n",
    "\n",
    "$$ f(x_1) = 0*1 + 0*1 = 0 $$ \n",
    "Prediction: 0 Label: 1. **Incorrect**, so update weight vector $w_1$: \n",
    "$$ w_{new} = [0, 0] + 1*[1, 1] = [1, 1] $$\n",
    "\n",
    "$$ f(x_2) = 1*2 + 1*(-1) = 1 $$ \n",
    "Prediction: 1 Label: -1. **Incorrect**, so update weight vector $w_1$: \n",
    "$$ w_{new} = [1, 1] + (-1)*[2, -1] = [-1, 2] $$\n",
    "\n",
    "$$ f(x_3) = (-1)*(-1) + 2*2 = 5 $$\n",
    "Prediction: 1 Label: 1. Correct, so do not update weight vector. \n",
    "\n",
    "### **Iteration 2** ### \n",
    "\n",
    "$$ f(x_2) = (-1)*1 + 2*1 = 1 $$\n",
    "Prediction: 1 Label: 1. Correct, so do not update weight vector. \n",
    "\n",
    "$$ f(x_2) = (-1)*2 + 2*(-1) = -4 $$\n",
    "Prediction: -1 Label: -1. Correct, so do not update weight vector. \n",
    "\n",
    "$$ f(x_3) = (-1)*(-1) + 2*2 = 5 $$\n",
    "Prediction: 1 Label: 1. Correct, so do not update weight vector. \n",
    "\n",
    "### Convergence reached. Mistakes: 2. w*: [-1, 2] ###\n",
    "\n",
    "## ----------------------- End Proof ----------------------- ##\n",
    "\n",
    "### **Summary of Results:**\n",
    "\n",
    "$S_1$ and $S_2$ are two training sets which contain the same points, but with the position of $x_1$ and $x_3$ swapped respectively. We find that: \n",
    "\n",
    "$S_1$: \n",
    "- Mistakes = 1\n",
    "- w* = [-1, 2]\n",
    "\n",
    "$S_2$:\n",
    "- Mistakes = 2\n",
    "- w* = [-1, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470de289-6eb5-4029-b80f-c4f85ae5ee3a",
   "metadata": {},
   "source": [
    "# Problem 2 #\n",
    "\n",
    "<span style=\"color:lightgray\">\n",
    "[10 points] We have mainly focused on squared loss, but there are other interesting losses in machine learning. Consider the following loss function which we denote by $\\phi(z) = max(0, −z)$. Let $S$ be a training set $(x^1 ,y^1 ),...,(x^m ,y^m )$ where each $x^i \\in R^n$ and $y^i \\in {−1, 1}$. Consider running stochastic gradient descent (SGD) to find a weight vector $w$ that minimizes $\\frac{1}{m}\\sum_{i=1}^{m}\\phi(y^i\\cdot w^Tx^i)$. Explain the explicit relationship between this algorithm and the Perceptron algorithm. Recall that for SGD, the update rule when the $i^{th}$ example is picked at random is\n",
    "\n",
    "$$ w_{new} = w_{old} - \\eta \\nabla \\phi(y^iw^Tx^i) $$\n",
    "\n",
    "Note: You do not need to be overly concerned about the discontinuity at ϕ(0), so you can ignore this when calculating the gradient for this problem.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac87a1fc-0168-4e12-9cfd-b5a775caaf1b",
   "metadata": {},
   "source": [
    "## **Solution**\n",
    "\n",
    "The Perceptron algorithm and the algorithm described above are explicitly related in two ways:\n",
    "- One, the weight vector is only updated when a mistake is made. This is because the loss function provided will only return a non-zero value when a mistake is made and the Perceptron works this way by design.\n",
    "- Two, the update rules for the algorithms are nearly identical. The only difference is the step-size variable, eta, in the SGD rule, as shown below:\n",
    "\n",
    "**Perceptron Update Rule:**       $$w_{new} = w_{old} + y^i\\cdot x^i$$\n",
    "**SGD Algorithm Update Rule:** $$w_{new} = w_{old} + \\eta\\; y^i\\cdot x^i$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Below I will show how I derived this update rule for $\\phi(z) = max(0, −z)$:\n",
    "\n",
    "---\n",
    "\n",
    "## ----------------------- Begin Proof ----------------------- ##\n",
    "\n",
    "The initial update rule for this loss function:\n",
    "\n",
    "$$ w_{new} = w_{old} - \\eta\\; \\nabla \\phi (y^iw^Tx^i) $$\n",
    "\n",
    "Using the partial derivative notation to illustrate the next step:\n",
    "\n",
    "$$ w_{new} = w_{old} - \\eta\\; \\frac{\\partial f}{\\partial w}(y^iw^Tx^i) $$\n",
    "\n",
    "Since $\\phi(z) = max(0, −z)$, and $z = y^i \\cdot w^T \\cdot x^i$, using the chain rule we have:\n",
    "\n",
    "$$ \\nabla_w\\phi(z) = - y^i \\cdot x^i\\;\\;\\; if\\;z\\lt 0 $$\n",
    "\n",
    "After plugging back into the update rule:\n",
    "\n",
    "$$ w_{new} = w_{old} + \\eta\\; y^i\\cdot x^i$$\n",
    "\n",
    "## ----------------------- End Proof ----------------------- ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb9abbf-ffa4-4a16-b361-f0bdfede7aa3",
   "metadata": {},
   "source": [
    "# Problem 3 #\n",
    "\n",
    "<span style=\"color:lightgray\">\n",
    "[10 points] Consider an iteration of the AdaBoost algorithm (using notation from the video lecture on Boosting) where we have obtained classifer $h_t$ . Show that with respect to the distribution $D_{t+1}$ generated for the next iteration, $h_t$ has accuracy exactly 1/2.\n",
    "</span>}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9091701-0b0b-4f3e-af52-a32622e97da8",
   "metadata": {},
   "source": [
    "If..\n",
    "\n",
    "The weight of the correct points after iteration $t = (\\frac{1}{2}+\\gamma)*\\beta*W$\\\n",
    "The weight of the correct points after iteration $t = (\\frac{1}{2}-\\gamma)*W$\n",
    "\n",
    "<br>\n",
    "\n",
    "The new sum of all the weights is:\n",
    "\n",
    "$$W*(2(\\frac{1}{2}-\\gamma))$$\n",
    "\n",
    "And after $i$ iterations, the sum of the weights is:\n",
    "\n",
    "$$W*(2(\\frac{1}{2}-\\gamma))^i$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Since the weights for Adaboost are updated as such:**\n",
    "For each $x_i$ such that $h_i(x_i)$ is correct:\n",
    "$$w_i^{new} = \\beta w_i^{old}$$\n",
    "For each $x_i$ such that $h_i(x_i)$ is incorrect:\n",
    "$$w_i^{new} = w_i^{old}$$\n",
    "\n",
    "#### **And the value of beta is such:**\n",
    "$$\\beta$ = $\\frac{E}{A}$$\n",
    "where\\\n",
    "$E$ = error rate\\\n",
    "$A$ = accuracy = $1-E$\n",
    "\n",
    "Such that:\n",
    "\n",
    "$$E = \\frac{1}{2}-\\gamma$$\n",
    "$$\\beta = \\frac{\\frac{1}{2}-\\gamma}{\\frac{1}{2}+\\gamma}$$\n",
    "\n",
    "#### **The accuracy can be calculated by...**\n",
    "Accuracy of $h_t$: \n",
    "$$h_t = 1-E_t = \\frac{1}{2} + \\gamma$$\n",
    "\n",
    "After $h_t$ is obtained, Adaboost will update the weights to form the new distribution, $D_{t+1}$, depending on if $h_t$ classified each point correctly or incorrectly. (See the weight update rules I defined above).\n",
    "\n",
    "After reweighting, the incorrectly classified examples will have a larger influence and correctly classified examples have a smaller influence. As a result **the classifier, which previously had accuracy** $\\frac{1}{2}+\\gamma$ **will now perform equally well on both correctly and incorrectly labeled examples, thus the accuracy with respect to** $D_{t+1} = \\frac{1}{2}$."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
