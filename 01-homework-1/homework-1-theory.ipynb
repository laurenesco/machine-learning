{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9695faa-93d7-42e8-95c1-652bf7a14604",
   "metadata": {},
   "source": [
    "# **Problem 1** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f76e02d-3a9c-4933-b90c-c981d2e81466",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# **Problem 2** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590934d9-6c28-46e6-bd29-b3b3f68a9f89",
   "metadata": {},
   "source": [
    "## Question: ##\n",
    "\n",
    "Let f be a decision tree with t leaves over the variables $x = (x_1, . . . , x_n) ∈\n",
    "{−1, 1}^n$. Explain how to write f as a multivariate polynomial $p(x_1, . . . , x_n)$ such that for every input$ x ∈ {−1, 1}n, f (x) = p(x)$ (You may interpret −1 as FALSE and 1 as TRUE or the other way round, at your preference.) \n",
    "\n",
    "(Hint: try to come up with an “indicator polynomial” for every leaf, i.e. one that evaluates to the leaf ’s value if x is such that that path is taken, and 0 otherwise.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e947df-de5e-4201-afa4-e77096b849a2",
   "metadata": {},
   "source": [
    "## Solution: ##\n",
    "\n",
    "For each leaf $L_i$, we will consider an indicator polynomial that is equal to the value of the leaf $v_i$ if the path to the leaf is taken, and 0 otherwise.\n",
    "\n",
    "Each leaf $L_i$ can be reached by a specific combination of variable values is established, e.g., the path to $L_i$ may require that $x_1 = 1$ and $x_2 = -1$. In this case, the path to $L_i$ is valid in this case if:\n",
    "\n",
    "- $x_1 = 1$\n",
    "- $x_2 = -1$\n",
    "\n",
    "So, the indicator polynomial would be:\n",
    "\n",
    "$$ p_{leaf} = \\frac{(1+x_1)}{2}*\\frac{(1-x_2)}{2} $$\n",
    "\n",
    "Generalizing this:\n",
    "\n",
    "$$ p(x) = \\sum_{leaves}*p_{leaf}(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f69402-fb90-4594-9f20-4e6a4d666033",
   "metadata": {},
   "source": [
    "# **Problem 3** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc22bd1d-cc58-409e-82b8-771f1d38baab",
   "metadata": {},
   "source": [
    "Compute a depth-two decision tree for the training data in table 1 using the Gini\n",
    "function, C(a) = 2a(1 − a) as described in class. What is the overall accuracy on the training\n",
    "data of the tree? For clarity, this will be a full binary tree and a full binary tree of depth-two has four leaves.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "233bf9c7-3b62-4939-98ee-0f17b40fa72a",
   "metadata": {},
   "source": [
    "# **Problem 4** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c35cc-3b4a-48e2-af73-bca93904c8c3",
   "metadata": {},
   "source": [
    " Suppose the domain X is the real line, R, and the labels lie in $Y = {−1, 1}$,\n",
    "Let C be the concept class consisting of simple threshold functions of the form $hθ$ for some\n",
    "$θ ∈ R$, where $hθ(x) = −1 \\;for\\; all\\; x ≤ θ$ and $hθ(x) = 1$ otherwise. Give a simple and efficient\n",
    "PAC learning algorithm for C that uses only $m = O( \\frac{1}{\\epsilon}log\\frac{1}{\n",
    "δ} ) $ training examples to output a\n",
    "classifier with error at most ϵ with probability at least 1 − δ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f560d-eee8-4849-9676-542e79388b56",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c2682390-b7b0-4ba7-ae1a-3c173fe4e266",
   "metadata": {},
   "source": [
    "# **Problem 5** #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7810fe8-d732-4731-983a-d957b0c9ff92",
   "metadata": {},
   "source": [
    "In this problem we will show that the existence of an efficient mistake-bounded\n",
    "learner for a class C implies an efficient PAC learner for C.\n",
    "Concretely, let C be a function class with domain X ∈ {−1, 1}n and binary labels Y ∈ {−1, 1}.\n",
    "Assume that C can be learned by algorithm/learner A with some mistake bound t. You may\n",
    "assume you know the value t. You may also assume that at each iteration, A runs in time\n",
    "polynomial in n and that A only updates its state when it gets an example wrong. The\n",
    "concrete goal of this problem is to create a PAC-learning algorithm, B, that can PAC-\n",
    "learn concept class C with respect to an arbitrary distribution D over {−1, 1}n using algorithm A as a sub-routine.\n",
    "\n",
    "In order to prove that learner B can PAC-learn concept class C, we must show that there exists\n",
    "a finite number of examples, m, that we can draw from D such that B produces a hypothesis\n",
    "whose true error is more than ϵ with probability at most δ. First, fix some distribution D on\n",
    "X, and we will assume that the examples are labeled by an unknown c ∈ C. Additionally,\n",
    "for a hypothesis (i.e. function) h : X → Y , let $err(h) = Px∼D[h(x)̸ = c(x)]$. Formally, we will need to bound m such that the following condition holds:\n",
    "\n",
    "$$ ∀δ, ϵ ∈ [0, 1], ∃m ∈ N | Px∼D[err(B({x}m)) > ϵ] ≤ δ x ∼ D (1) $$\n",
    "\n",
    "where B({x}m) denotes a hypotheses produced from B with m random draws of x from an\n",
    "arbitrary distribution D.\n",
    "\n",
    "To find this m, we will first decompose it into blocks of examples of size k and make use of\n",
    "results based on a single block to find the bound necessary for m that satisfies condition 1.\n",
    "Note: Using the identity $P[err(h) > ϵ]+P[err(h) ≤ ϵ] = 1$, we can see that $P[err(h) > ϵ] ≤ δ ⇔ P[err(h) ≤ ϵ] ≥ 1 − δ $, which makes the connection to the definition of PAC-learning discussed in lecture explicit.\n",
    "\n",
    "(a) Fix a single arbitrary hypothesis h′ : X → Y produced by A and determine a lower\n",
    "bound on the number of examples, k, such that $P[err(h′) > ϵ] ≤ δ′$. (The contrapositive\n",
    "view would be: with probability at least 1 − δ′, it must be the case that err(h′) ≤ ϵ.\n",
    "Make sure this makes sense.)\n",
    "\n",
    "(b) From part 5a we know that as long as a block is at least of size k, then if that block is\n",
    "classified correctly by a fixed arbitrary hypothesis h′ we can effectively upper bound the\n",
    "probility of the ‘bad event’ (i.e. A outputs h′ s.t. $err(h′) > ϵ)$ by δ′. However, our bound\n",
    "must apply to every h that our algorith B could output for an arbitrary distribution\n",
    "D over examples. With this in mind, how large should m be so that we can bound\n",
    "all hypotheses that could be output? (You may assue that algorithm B will know the\n",
    "mistake bound throughout the question.)\n",
    "\n",
    "(c) Put everything together and fully describe (with proof) a PAC learner that is able to\n",
    "output a hypothesis with a true error at most ϵ with probability at least 1 − δ, given\n",
    "a mistake bounded learner A. To do this you should first describe your pseudocode for\n",
    "algorithm B which will use A as a sub-routine (no need for minute details or code, broad\n",
    "2 strokes will suffice). Then, prove there exists a finite number of m examples for B to\n",
    "PAC-learn C for all values of δ and ϵ by lower bounding m by a function of ϵ, δ, and t\n",
    "(i.e. finding a finite lower bound for m such that the PAC-learning requirements in 1\n",
    "are satisfied)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35e39f4d-61f0-448c-aa20-0cbf80eac511",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
