{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# 2.2 Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "**frequentist view**\n",
    "- a framework for statistical estimation, the view of estimating unknown but fixed parameters from randomized data sets.\n",
    "\n",
    "**subjective absolute**\n",
    "\n",
    "**posterier distribution**\n",
    "\n",
    "**prior**\n",
    "- a distribution that we assign to a parameter if we don't observe any information about the parameter.\n",
    "\n",
    "**gaussian probability formula**\n",
    "$$ P(x_i|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma_1}exp(-\\frac{(x_i-\\theta)^2}{2\\sigma_1^2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {},
   "source": [
    "## ***2.2.0 Bayesian Inference***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7e2ec31-1e8c-44c3-bb67-c6f2b821879c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Introduction**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Bayesian inference is a powerful set of techniques for parameter estimation. It has the advantage of incorporating prior information as well as quantifying uncertainty. While MLE is a frequentist view, bayesian inference is a significantly different view.\n",
    "\n",
    "In Bayesian inference, they key idea is that the unknown parameter **is not** viewed as a deterministic value. It is viewed as a random variable. \n",
    "\n",
    "The idea behind this is that, even though theta is actually fixed, we don't actually observe theta directly. We only have very limited, partial information about theta. That is something called a **subjective absolute**. Essentially, to us, theta is a random variable.\n",
    "\n",
    "In Bayesian inference, we are going to assume theta is a random variable, then we are going to explicitly calculate its posterier distribution given observation, using what's called Bayes' rule.\n",
    "\n",
    "#### **Bayes' Rule**\n",
    "\n",
    "First, the posterier distribution is notated as:\n",
    "\n",
    "$$ p(\\theta|D)$$\n",
    "\n",
    "And the likelihood function is notated as:\n",
    "\n",
    "$$ p(D|\\theta) $$\n",
    "\n",
    "And the prior (see vocabulary section) is notated as:\n",
    "\n",
    "$$ p(\\theta) $$\n",
    "\n",
    "And the marginal distribution of the data is notated as:\n",
    "\n",
    "$$ \\int p(D|\\theta)\\;(p(\\theta)\\;d\\theta $$\n",
    "\n",
    "Putting it all together, Bayes' rule is as follows:\n",
    "\n",
    "$$ p(\\theta|D) = \\frac{p(D|\\theta)\\;p(\\theta)}{p(D)} $$\n",
    "\n",
    "Where the marginal distribution is being used as a normalization constant to ensure the posterier distribution is normalized to have an integration of 1.\n",
    "\n",
    "#### **Proving Bayes' Rule**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **Bayesian Inference Illustrative Example**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "In this example, we see that $\\theta$ is a binary variable that outputs 1 if the sun exploded, and 0 otherwise. $x$ is also a binary variable that outputs 1 if the alarm goes off and 0 otherwise.\n",
    "\n",
    "If the alarm fires, do we believe the device? We have two conflicting pieces of evidence. \n",
    "1. This device is very accurate, with $\\alpha = 0.0001$ in this case (professor set).\n",
    "2. The likelihood of the sun exploding today or any other day is infintesimally small.\n",
    "\n",
    "How can we combine these two pieces of evidence? Luckily, that is what bayesian inference can do.\n",
    "\n",
    "---\n",
    "\n",
    "First let's try MLE, which we find will not work in this case:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "The reason why it fails is because we only have one data point, and we do not use any other prior knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "Now, using Bayesian inference:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Thus, the decision of whether $\\theta$ should be 0 or 1 can be written as:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.6.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "So we can say we should predict $\\theta = 1$ if:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.7.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Predicting if $\\theta$ should be 0 can be derived the same way. In this case $\\theta$ is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed5cd-43a9-4384-be36-0a59ee244e86",
   "metadata": {},
   "source": [
    "## ***2.2.1 More Examples***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "10fb82a9-1780-4c14-9406-a59c82ba1d90",
   "metadata": {},
   "source": [
    "Recall the formula for Bayesian Inference:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.8.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Note that since the normalization constant, $P(D)$, does not rely on $\\theta$, the Bayesian Inference formula is proportional to the formula on the right for the purpose of estimating $\\theta$.\n",
    "\n",
    "#### **Example 1**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.9.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "In this example we would like to predict the commute time at our new apartment. We have some **prior knowledge** from our friend about the commute time, and we have a few **observations** we have made ourselves while testing the drive. \n",
    "\n",
    "In this problem, $\\theta$ is the commute time. We can use a Gaussian distribution to capture the prior knowledge, where $p(\\theta) \\textasciitilde \\mathcal{N}(\\mu_0, \\sigma^2_0)$, and $\\mu_0 = 30$, and $\\sigma_0 = 10$. Our observations can be denoted $x_1, \\dots, x_n$, where $x_i = \\theta + \\sigma_1 \\xi_i$, $\\xi_i \\textasciitilde \\mathcal{N}(0,1)$, $\\sigma_1 = 5$ (in practice you can estimate sigma).\n",
    "\n",
    "Based on the assumption that we know $\\sigma_1$, we can actually define the likelihood function:\n",
    "\n",
    "$$ P(x_i|\\theta)\\textasciitilde \\mathcal{N}(\\theta,\\sigma_1^2) $$\n",
    "\n",
    "Then, to estimate the posterior distribution:\n",
    "\n",
    "$$ P(\\theta|D) = \\frac{P(D|\\theta)\\;P(\\theta)}{P(D)}  \\propto P(D|\\theta)\\;P(\\theta)$$\n",
    "\n",
    "Now, since our observations are a set of data points, we will take the product like we did for MLE:\n",
    "\n",
    "$$ P(\\theta|D) = [\\prod_{i=1}^nP(x_i|\\theta)]\\;P(\\theta) $$\n",
    "\n",
    "Then, since we decided that the observations were Gaussian, we can use the formula for a Gaussian (see Vocabulary section). Luckily, since the first term in the formula is a constant since we are estimating $\\theta$, we can ignore it, thus:\n",
    "\n",
    "$$ P(x_i|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma_1}exp(-\\frac{(x_i-\\theta)^2}{2\\sigma_1^2}) \\propto exp(-\\frac{(x_i-\\theta)^2}{2\\sigma_1^2})$$\n",
    "\n",
    "So the posterior distribution will be:\n",
    "\n",
    "$$ \\propto [\\prod_{i=1}^n exp(-\\frac{(x_i-\\theta)^2}{2\\sigma_1^2})] exp(-\\frac{(\\theta-\\mu_0)^2}{2\\sigma_0^2}) $$\n",
    "\n",
    "$$ \\propto exp(-\\sum_{i=1}^n \\frac{(\\theta-x_i)^2}{2\\sigma_1^2} -\\frac{(\\theta-\\mu_0)^2}{2\\sigma_0^2}) $$\n",
    "\n",
    "$$ = exp(-\\frac{1}{2}(A\\theta^2-2B\\theta+C)) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {},
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
