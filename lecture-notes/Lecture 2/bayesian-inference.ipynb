{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# 2.2 Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "**frequentist view**\n",
    "- a framework for statistical estimation, the view of estimating unknown but fixed parameters from randomized data sets.\n",
    "\n",
    "**subjective absolute**\n",
    "\n",
    "**posterier distribution**\n",
    "\n",
    "**prior**\n",
    "- a distribution that we assign to a parameter if we don't observe any information about the parameter.\n",
    "\n",
    "**gaussian probability formula**\n",
    "$$ P(x_i|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma_1}exp(-\\frac{(x_i-\\theta)^2}{2\\sigma_1^2}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***2.2.0 Bayesian Inference***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7e2ec31-1e8c-44c3-bb67-c6f2b821879c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Introduction**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Bayesian inference is a powerful set of techniques for parameter estimation. It has the advantage of incorporating prior information as well as quantifying uncertainty. While MLE is a frequentist view, bayesian inference is a significantly different view.\n",
    "\n",
    "In Bayesian inference, they key idea is that the unknown parameter **is not** viewed as a deterministic value. It is viewed as a random variable. \n",
    "\n",
    "The idea behind this is that, even though theta is actually fixed, we don't actually observe theta directly. We only have very limited, partial information about theta. That is something called a **subjective absolute**. Essentially, to us, theta is a random variable.\n",
    "\n",
    "In Bayesian inference, we are going to assume theta is a random variable, then we are going to explicitly calculate its posterier distribution given observation, using what's called Bayes' rule.\n",
    "\n",
    "#### **Bayes' Rule**\n",
    "\n",
    "First, the posterier distribution is notated as:\n",
    "\n",
    "$$ p(\\theta|D)$$\n",
    "\n",
    "And the likelihood function is notated as:\n",
    "\n",
    "$$ p(D|\\theta) $$\n",
    "\n",
    "And the prior (see vocabulary section) is notated as:\n",
    "\n",
    "$$ p(\\theta) $$\n",
    "\n",
    "And the marginal distribution of the data is notated as:\n",
    "\n",
    "$$ \\int p(D|\\theta)\\;(p(\\theta)\\;d\\theta $$\n",
    "\n",
    "Putting it all together, Bayes' rule is as follows:\n",
    "\n",
    "$$ p(\\theta|D) = \\frac{p(D|\\theta)\\;p(\\theta)}{p(D)} $$\n",
    "\n",
    "Where the marginal distribution is being used as a normalization constant to ensure the posterier distribution is normalized to have an integration of 1.\n",
    "\n",
    "#### **Proving Bayes' Rule**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **Bayesian Inference Illustrative Example**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "In this example, we see that $\\theta$ is a binary variable that outputs 1 if the sun exploded, and 0 otherwise. $x$ is also a binary variable that outputs 1 if the alarm goes off and 0 otherwise.\n",
    "\n",
    "If the alarm fires, do we believe the device? We have two conflicting pieces of evidence. \n",
    "1. This device is very accurate, with $\\alpha = 0.0001$ in this case (professor set).\n",
    "2. The likelihood of the sun exploding today or any other day is infintesimally small.\n",
    "\n",
    "How can we combine these two pieces of evidence? Luckily, that is what bayesian inference can do.\n",
    "\n",
    "---\n",
    "\n",
    "First let's try MLE, which we find will not work in this case:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "The reason why it fails is because we only have one data point, and we do not use any other prior knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "Now, using Bayesian inference:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Thus, the decision of whether $\\theta$ should be 0 or 1 can be written as:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.6.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "So we can say we should predict $\\theta = 1$ if:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.7.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Predicting if $\\theta$ should be 0 can be derived the same way. In this case $\\theta$ is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed5cd-43a9-4384-be36-0a59ee244e86",
   "metadata": {},
   "source": [
    "## ***2.2.1 More Examples***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ecb970-af51-42c1-bd35-5d45b6b21730",
   "metadata": {},
   "source": [
    "Recall the formula for Bayesian Inference:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.8.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Note that since the normalization constant, $P(D)$, does not rely on $\\theta$, the Bayesian Inference formula is proportional to the formula on the right for the purpose of estimating $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b7053d-5214-42a5-97df-8caeada8dfdb",
   "metadata": {},
   "source": [
    "### **Example 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1408ac-c76d-4175-90d4-8250c59b3b62",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.9.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "In this example we would like to predict the commute time at our new apartment. We have some **prior knowledge** from our friend about the commute time, and we have a few **observations** we have made ourselves while testing the drive. \n",
    "\n",
    "In this problem, $\\theta$ is the commute time. We can use a Gaussian distribution to capture the prior knowledge, where $p(\\theta) \\textasciitilde \\mathcal{N}(\\mu_0, \\sigma^2_0)$, and $\\mu_0 = 30$, and $\\sigma_0 = 10$. Our observations can be denoted $x_1, \\dots, x_n$, where $x_i = \\theta + \\sigma_1 \\xi_i$, $\\xi_i \\textasciitilde \\mathcal{N}(0,1)$, $\\sigma_1 = 5$ (in practice you can estimate sigma).\n",
    "\n",
    "Based on the assumption that we know $\\sigma_1$, we can actually define the likelihood function:\n",
    "\n",
    "$$ P(x_i|\\theta)\\textasciitilde \\mathcal{N}(\\theta,\\sigma_1^2) $$\n",
    "\n",
    "Then, to estimate the posterior distribution:\n",
    "\n",
    "$$ P(\\theta|D) = \\frac{P(D|\\theta)\\;P(\\theta)}{P(D)}  \\propto P(D|\\theta)\\;P(\\theta)$$\n",
    "\n",
    "Now, since our observations are a set of data points, we will take the product like we did for MLE:\n",
    "\n",
    "$$ P(\\theta|D) = [\\prod_{i=1}^nP(x_i|\\theta)]\\;P(\\theta) $$\n",
    "\n",
    "Then, since we decided that the observations were Gaussian, we can use the formula for a Gaussian (see Vocabulary section). Luckily, since the first term in the formula is a constant since we are estimating $\\theta$, we can ignore it, thus:\n",
    "\n",
    "$$ P(x_i|\\theta) = \\frac{1}{\\sqrt{2\\pi}\\sigma_1}exp(-\\frac{(x_i-\\theta)^2}{2\\sigma_1^2}) \\propto exp(-\\frac{(x_i-\\theta)^2}{2\\sigma_1^2})$$\n",
    "\n",
    "So the posterior distribution will be:\n",
    "\n",
    "$$ \\propto [\\prod_{i=1}^n exp(-\\frac{(x_i-\\theta)^2}{2\\sigma_1^2})] exp(-\\frac{(\\theta-\\mu_0)^2}{2\\sigma_0^2}) $$\n",
    "\n",
    "$$ \\propto exp(-\\sum_{i=1}^n \\frac{(\\theta-x_i)^2}{2\\sigma_1^2} -\\frac{(\\theta-\\mu_0)^2}{2\\sigma_0^2}) $$\n",
    "\n",
    "We can see that this expression is an exponential of some quadratic function about $\\theta$, so to simplify it, we can write this as a quadratic function of $\\theta$:\n",
    "\n",
    "$$ = exp(-\\frac{1}{2}(A\\theta^2-2B\\theta+C)) $$\n",
    "\n",
    "We can now solve for $A, B,$ and $C$ by looking back at the posterior we solved for:\n",
    "\n",
    "$$ A = \\sum_{i=1}^n\\frac{1}{\\sigma^2} $$\n",
    "$$ A = \\frac{n}{\\sigma^2}+\\frac{1}{\\sigma^2} $$\n",
    "$$ B = \\sum_{i=1}^n \\frac{x_i}{\\sigma_1^2} + \\frac{\\mu_0}{\\sigma^2_o} $$\n",
    "$$ C = do\\;not\\;care,\\;it\\;is\\;a\\;constant $$\n",
    "\n",
    "Back to further simplifying the quadratic equation we created:\n",
    "\n",
    "$$ exp(-\\frac{1}{2} A(\\theta=\\frac{B}{A})^2+const) $$\n",
    "\n",
    "And we can view this as the likelihood of a Gaussian distribution:\n",
    "\n",
    "$$ \\sim \\mathcal{N}(\\frac{B}{A}, \\frac{1}{A}) $$\n",
    "\n",
    "Using the above, and our solved values for $A$ and $B$:\n",
    "\n",
    "$$ \\mu_p = \\frac{B}{A} = \\frac{\\sum_{i=1}^n \\frac{x_i}{\\sigma_1^2} + \\frac{\\mu_0}{\\sigma^2_o}}{\\frac{n}{\\sigma^2}+\\frac{1}{\\sigma^2}} $$\n",
    "\n",
    "$$ \\sigma_p^2 = \\frac{1}{A} = (\\frac{n}{\\sigma^2}+\\frac{1}{\\sigma^2})^{-1} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8ba78e-c357-4d57-8270-a8aab59d1de2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Example 2**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b30382f-4664-4bb5-ab41-a9ef616ca9aa",
   "metadata": {},
   "source": [
    "#### **Setup**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.10.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "In this case we will be using Bayesian inference to solve linear regression, which is typically solved using least squares estimation. However, Bayesian inference has the advantage of quantifying the uncertainty in the data and the parameter estimation.\n",
    "So if we want an uncertainty estimation about how accurate our estimate of $\\theta$ might be, this is what Bayesian inference can provide.\n",
    "\n",
    "#### **Initialize the Prior Distribution**\n",
    "\n",
    "In this case, we will treat $\\theta$ as a random variable, and assume the prior is a Gaussian distribution. In reality, you must decide what the prior will be, but we can typically assume a Gaussian so that is what we are using for this problem. $\\mu_0$ and $\\sigma_0$ need to be set by us, and this could come from some prior information. Or, if you don't have a lot of information, we can assume $\\mu_0 = 0$ and $\\sigma_0$ is some very large number.\n",
    "\n",
    "**TLDR;** If you have information about $\\theta$, you can set a sharp prior. Otherwise, you can set a very generalized and uninformative prior such as $\\mu_0 = 0$ and $\\sigma_0 =$ some very large number.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.12.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "\n",
    "#### **Determining the Likelihood**\n",
    "\n",
    "We can assume the $y_i$ is equal to our $x_i$ times $\\theta$ plus potentially some Gaussian noise:\n",
    "\n",
    "$$ y_i = x_i^T \\theta + \\sigma_i\\xi_i $$\n",
    "\n",
    "Where $\\sigma_1$ is some variance that we set or derive from the data, and $\\xi_i$ is some standard Gaussian noise: $\\xi_i \\sim \\mathcal{N}(0,1)$.\n",
    "\n",
    "Then we can write the likelihood as:\n",
    "\n",
    "$$ P(\\{y_i, x_i\\}\\mid\\theta) $$\n",
    "\n",
    "Simplifying using the chain rule:\n",
    "\n",
    "$$ = P(y_i\\mid x_i, \\theta)\\;P(x_i) $$\n",
    "\n",
    "And since the $P(x_i)$ is a constant in terms of $\\theta$, we can ignore it in further calculations.\n",
    "\n",
    "#### **Finding the Posterior Distribution**\n",
    "\n",
    "Plugging in the probabilities (recall the data points were i.i.d.), then simplifying using the chain rule:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.13.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Then, since we used a Gaussian prior and a Gaussian likelihood, we can use the Gaussian probability formula:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.2.14.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Where we turn the prior into a quadratic equation in order to isolate and solve for $\\mu$ and $\\sigma^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {},
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
