{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# Kernel Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "**kernel function**\n",
    "- a two variable function that is symmetric: $K(x, x') = K(x', x)$\n",
    "\n",
    "**basis function**\n",
    "- A basis function is a mathematical function used to transform input data into a new representation, typically in a higher-dimensional space, to help capture complex patterns or relationships\n",
    "\n",
    "**interpolate**\n",
    "- the ability to exactly reporoduce the observed values of the dataset at the training points\n",
    "\n",
    "**positive semi-definite**\n",
    "- a square matrix $K$ is postivie semi-definite if\n",
    "1. it is symmetric ($K = K^T$)\n",
    "2. for any non-zero vectore $z$, the quadratic form $z^TKz \\ge 0$\n",
    "\n",
    "**nonparametric method**\n",
    "- a model whose complexity grows with the amount of data, as opposed to being fixed by a predefined number of parameters (e.g. the kernel method has parameters $\\theta_1, \\dots, \\theta_n$, where $n$ is the size of the dataset)\n",
    "\n",
    "**regularization**\n",
    "- a technique used to prevent overfitting by discouraging overly complex models. it achieves this by adding a penalty term to the loss function, reducing the magnitude of the models parameters or limiting their flexibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {},
   "source": [
    "## ***The Kernel Method***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc401538-1983-40c0-b231-f535e625a1cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0162d-0a47-49a1-8a85-f5cced366320",
   "metadata": {},
   "source": [
    "#### **What is a Kernel Method**\n",
    "\n",
    "The kernel method is a very important class of algorithms and methods for constructing very flexible, nonlinear functional approximations in machine learning. They can achieve this by mapping data into higher dimensional spaces using the kernel trick.\n",
    "\n",
    "#### **Reviewing Supervised Learning**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.6.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "We can see that in supervised learning, the three main steps are:\n",
    "1. Decide a function class $\\mathcal {F}$, which cannot be too large (overfitting), but must be large enough to consider the complexities of the data.\n",
    "2. Define a loss function for $\\mathcal{f} \\in \\mathcal{F}$\n",
    "3. Solve the optimization problem of finding the lowest loss for each $\\mathcal {f} \\in \\mathcal{F}$.\n",
    "\n",
    "Most supervised learning techniques fall into this exact framework, and they use a linear function class, such as:\n",
    "\n",
    "$$ \\mathcal{F} \\triangleq \\{\\mathcal{F}_\\theta (x) = \\sum_{l=1}^d \\theta_l x_l + \\theta_0 \\mid \\forall \\; \\theta_l \\in \\mathbb{R} \\}$$\n",
    "\n",
    "Linear function classes work well in many settings, but they struggle to model more complex patterns. This is where the kernel method (and later neural networks) come in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88b4a37-2098-4bea-a85b-b06e4d0ddd33",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **The Foundation of Kernel Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cae3b65-6e14-4d81-be67-f328e58d493b",
   "metadata": {},
   "source": [
    "#### **Basis Functions**\n",
    "\n",
    "Basis functions, $\\phi(x)$ can be viewed as mapping $x$ to some variable. An example of a basis function is the polynomial basis function:\n",
    "\n",
    "$$ \\phi_l(x0) = x^l \\implies 1, x, x^2, \\dots $$\n",
    "\n",
    "This example illustrates how input features are transformed into powers of $x$ to capture polynomial relationships.\n",
    "\n",
    "#### **A Basic and Naive Approach to Building Nonlinear Approximation**\n",
    "\n",
    "A very basic and naive way to incorporate nonlinear functions into $\\mathcal{f}$ is to replace the feature with a set of nonlinear basis functions. So\n",
    "\n",
    "$$ f(x, \\theta) = \\sum_{l=1}^d \\theta_l x_l = \\theta^\\intercal x $$\n",
    "\n",
    "becomes\n",
    "\n",
    "$$ f(x, \\theta) = \\sum_{l=1}^m \\theta_l \\phi_l(x) = \\theta^\\intercal \\phi(x) $$\n",
    "\n",
    "Where $\\phi(x) = [\\phi_1(x), \\dots, \\phi_m(x)]^\\intercal$ is a set of nonlinear basis functions believed to capture important nonlinear patterns regarding the input, and $\\theta = [\\theta_1, \\dots, \\theta_m]^\\intercal$ is the coefficient vector. For a fixed $phi(x)$, the coefficient $\\theta$ is estimated the same way as linear regression, so this has a simple closed form solution. (It is important to note that higher dimensions of $\\phi(x)$ can still make computations challenging)\n",
    "\n",
    "The problem with this approach is that you must manually decide what type of basis function to use. This can be very problem dependent, and hand crafting different functions each time can be problematic.\n",
    "\n",
    "This is a very simple implementation, but the **key idea of converting the input space using a nonlinear transform into another input space, them performing linear regression on it** is fundamental to nonlinear approximations. \n",
    "\n",
    "#### **Building Flexible and Adaptive Nonlinear Approximation**\n",
    "\n",
    "Our goal is to have an algorithm that can **automatically construct a set of basis functions based on the data**, which we can then do linear regression on. This is called an **adaptive basis function**. Both the kernel method and neural networks are methods to find adaptive basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c15dc1-fb91-4da5-9f95-3f57199581c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Putting it all Together**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb830fe-68fa-4484-9634-da708efc459e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **The Kernel Method in Practice**\n",
    "\n",
    "Let's say we have a set of data points, $\\mathcal{D}$, that is two-dimensional. The idea of kernel methods is that we can represent each of the data points using its similarity with all the other data points. In the example below, the data point \"star\" can be characterized/identified by its similarity with all the other points. Further, the more and more data we have, the more dimensions we can use to represent this data point, the more flexible of a representation we have.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.6.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **Generating a Similarity Score**\n",
    "\n",
    "In order to do this, we need to define a **similarity function**. This function takes two points and calculates a similarity score betwen them, a real number:\n",
    "\n",
    "$$ K(x,x') : X \\times X \\mapsto \\mathbb{R} $$\n",
    "\n",
    "Often we use Gaussian similarity functions, for example the Gaussian radial basis function (RBF) kernel:\n",
    "\n",
    "$$K(x, x') = exp(-\\frac{1}{2h^2}||x-x'||^2_2) $$\n",
    "\n",
    "Where $h$ is a hyperparameter called **bandwidth**, which controls how quickly similarity decays with distance, and $||x-x'||^2_2$ is the Euclidean distance between $x$ and $x'$. $h$ needs to be decided by the programmer, and significantly affects the flexibility of the model. It is sometimes taken to be the variance of observed distribution, but is regardless typically tuned through cross-validation.\n",
    "\n",
    "#### **Generating a Feature Map**\n",
    "\n",
    "We can define a feature map, a set of basis functions, that use the similarity function we created:\n",
    "\n",
    "$$ \n",
    "\\phi(x) =\n",
    "\\begin{bmatrix}\n",
    "k(x, x_1) \\\\\n",
    "k(x, x_2) \\\\\n",
    "\\vdots \\\\\n",
    "k(x, x_n)\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "Note that $\\phi(x)$ grows with the size of the dataset. Now we can use this representation to construct the linear regression:\n",
    "\n",
    "$$ \\mathcal{f}_\\theta (x) = \\theta^\\intercal \\phi (x) = \\sum_{i=1}^n \\theta_i \\phi_i (x) $$\n",
    "\n",
    "$$ = \\sum_{i=1}^n \\theta_i K(x,x_i) $$\n",
    "\n",
    "Where $\\theta$ is a vector of the same size as the training set:\n",
    "\n",
    "$$ \n",
    "\\theta =\n",
    "\\begin{bmatrix}\n",
    "\\theta_1 \\\\\n",
    "\\theta_2 \\\\\n",
    "\\vdots \\\\\n",
    "\\theta_n\n",
    "\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "There will be a $\\theta_i$ associated with each point in the dataset which measures the importance of each of the data points. This is estimated from the data by minimizing the loss function.\n",
    "\n",
    "This formula, $\\mathcal{f}_\\theta (x) = \\sum_{i=1}^n \\theta_i K(x,x_i)$ takes a sum over all the data points, measuring the similarity from $x$ to each of the data points, weighted by some parameter, $\\theta_i$. This allows us to construct a flexible class of functions.\n",
    "\n",
    "#### **Visualizing the Concept**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=100%\" src=\"images/2.6.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **Key Properties**\n",
    "\n",
    "There are two nice properties to this approach:\n",
    "1. the construction of basis functions is adapted with vector training data\n",
    "2. if we have more data the function class $\\mathcal{F}$ will be even more flexible.\n",
    "\n",
    "Note that $\\mathcal{F}$ becomes more flexible as $n$ increases because the amount of parameters ($\\theta_i$) is equal to the size of the training set, $n$. However, the dimension of $\\mathcal{F}$ is also equal to $n$, so larger datasets can lead to computational challenges and cost. This makes the kernel method a **nonparametric method**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1577e8e4-73b2-4620-8bfc-5d2b4ec3d31c",
   "metadata": {},
   "source": [
    "### **Estimating Theta**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c13149f-1e4b-46ed-bee0-70cc9ef7572e",
   "metadata": {},
   "source": [
    "Once the model is set up, we can work on estimating $\\theta$. That is, we want to find the optimal $\\theta$ to minimize the loss function. In this case, the loss function is:\n",
    "\n",
    "$$ \\underset{\\theta}{min} \\;L(\\theta) = \\sum_{j=1}^n (y_j = \\sum_{i=1}^n \\theta_i K(x_j, x_i))^2 $$\n",
    "\n",
    "So for each data point $x_j$, we measure the square difference between $y_j$ and the kernel function evaluation at $x_j$.\n",
    "\n",
    "This loss function is equivalent to:\n",
    "\n",
    "$$ \\underset{\\theta}{min} \\;L(\\theta) = || Y - K\\theta||_2^2 $$\n",
    "\n",
    "Where $Y$ is the collection of labels, and $K$ is the pairwise similarities:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"100%\" src=\"images/2.6.4.png\" alt=\"Professor Notes\">\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "So we take the difference, take the norm and square it, and minizimize the square. This is a standard linear regression problem that we will solve by taking the derivate, which is:\n",
    "\n",
    "$$ \\nabla L(\\theta) = 2K^\\intercal (K\\theta - Y) $$\n",
    "\n",
    "Solving, making the important **assumption that $K$ is invertible**:\n",
    "\n",
    "$$ \\nabla L(\\theta) = 2K^\\intercal (K\\theta - Y) $$\n",
    "$$ \\implies K^\\intercal K\\theta = K^\\intercal Y $$\n",
    "$$ \\theta = K^{-1}Y $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c00e7b-ff2c-44eb-8af2-04b7aa1fea55",
   "metadata": {},
   "source": [
    "### **Preventing Overfitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068654f-5579-4a67-ae41-b50a72ed03e7",
   "metadata": {},
   "source": [
    "#### **Regularization**\n",
    "\n",
    "Since the amount of parameters is equal to the number of observed data points, overfitting is a concern for this model. However we can implement something called **regualrization** to prevent this.\n",
    "\n",
    "Instead of doing the exact linear regression, which can overfit the training data, we can solve a regularized version of the loss function. \n",
    "\n",
    "The original loss function\n",
    "\n",
    "$$ \\underset{\\theta}{min} \\;L(\\theta) = || Y - K\\theta||_2^2 $$\n",
    "\n",
    "Regularized version:\n",
    "\n",
    "$$ \\underset{\\theta}{min} \\;L(\\theta) = || Y - K\\theta||_2^2 + \\alpha \\Phi(\\theta)$$\n",
    "\n",
    "Where $\\Phi(\\theta)$ is some regularization term and $\\alpha$ is the regularization coefficient.\n",
    "\n",
    "#### **Aside: Understanding Phi and alpha**\n",
    "\n",
    "$\\Phi(\\theta)$ is the **regularization term**, it is a function of $\\theta$ that quantifies the models complexity. Different choices for $\\Phi(\\theta)$ lead to different types of regularization:\n",
    "\n",
    "**L2 Regularization (Ridge)**\n",
    "$$ \\Phi(\\theta) = ||\\theta||_2^2 = \\sum_{i=1}^n \\theta_i^2 $$\n",
    "- Penalizes the sum of squared coefficients\n",
    "- Encourages small but nonzero coefficients\n",
    "\n",
    "**L1 Regularization (Lasso)**\n",
    "\n",
    "- Penalizes the sum of absolute values of coefficients\n",
    "- Encourages sparsity (some $\\theta_i$ become exactly 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": [
    "#### **Kernel Representation Benefits**\n",
    "\n",
    "- Flexibility: Kernel functions can map data into spaces where complex relationships become linear.\n",
    "- Nonlinearity: Even if $x$ and $x′$ have nonlinear relationships in the input space, they may appear linearly separable in the kernel space.\n",
    "- Avoiding Explicit Basis Functions: The kernel trick allows computations to be performed directly with $k(x,x′)$ without explicitly defining or computing the high-dimensional $\\phi(x)$.\n",
    "\n",
    "#### **How to Choose x to Represent in Kernel Space**\n",
    "\n",
    "- In prediction tasks, you will represent the query point (the input for which you would like an output) as $x$ in $\\phi(x)$.\n",
    "- If you want to analyze the relationships amongst the dat points, you can represent each point $x_i \\in \\mathcal{D}$ in kernel space to explore its similarity to other points in $\\mathcal{D}$.\n",
    "\n",
    "#### **What is this \"Flexible Class of Functions\"**\n",
    "\n",
    "**Flexible class of functions** refers to the family of functions, including $\\mathcal{f}_\\theta (x) = \\sum_{i=1}^n \\theta_i K(x,x_i)$, which are defined by:\n",
    "- the kernel used to measure similarity (e.g., Gaussian RBF, Laplace)\n",
    "- the weights, $\\theta_i$ that control the influence of the similarity score\n",
    "\n",
    "This class is \"flexible\" because by choosing different kernels and different values for $\\theta_i$, we can generate many different functions.\n",
    "\n",
    "#### **Kernel Choice**\n",
    "The kernel $K(x,x')$ chosen defines the shape and properties of the functions in the generated class, for example:\n",
    "- a Laplace kernel produces a class of functions that are less smooth and more sensitive to small changes in $x$\n",
    "- a Gaussian RBF kernel produces a smoother, more globally stable class of functions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
