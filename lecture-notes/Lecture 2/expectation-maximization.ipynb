{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# Expectation Maximization "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "- **Joint Distribution:**\n",
    "    - The overall rule describing all combined outcomes of multiple variables.\n",
    "    - $Pr(X, Y)$\n",
    "- **Joint Probability:**\n",
    "    - The probability of a specific pair of outcomes happening together.\n",
    "    - $Pr(X = 1, Y = 1)$\n",
    "- **Joint Likelihood Function:**\n",
    "    - Evaluates how likely a set of observed outcomes is, given model parameters.\n",
    "    - $L(\\theta; x_1, y_1, \\ldots, x_n, y_n) = \\prod_{i=1}^n P(X = x_i, Y = y_i \\mid \\theta)$\n",
    "- **Marginal Probability**\n",
    "    - The probability of a single variable taking a specific value, obtained by summing or integrating over all possible values of other variables.\n",
    "    - Discrete: $P(X = x) = \\sum_y P(X = x, Y = y)$\n",
    "    - Continuous: $f_X(x) = \\int_{-\\infty}^{\\infty} f_{X,Y}(x, y) \\, dy$\n",
    "- **Impute**\n",
    "    - To replace missing or incomplete data with substituted values. This is commonly used in data analysis and machine learning when handling datasets with missing entries.\n",
    "- **Posterior Distribution**\n",
    "    - The updated probability distribution of a parameter after observing data, combining prior beliefs with the evidence from the data via Bayes' theorem.\n",
    "    - $P(\\theta \\mid X) = \\frac{P(X \\mid \\theta) \\, P(\\theta)}{P(X)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {},
   "source": [
    "## ***Part I***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc401538-1983-40c0-b231-f535e625a1cf",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf12106-76fa-4613-8765-29ce449e2334",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Background**\n",
    "\n",
    "Expectation maximization is the **application of maximum likelihood estimation applied to clustering problems.** It is a more pricipled and powerful way to perform the same task as the K-means algorithm, with the advantage of quantifying uncertainty. This technique is used to **estimate parameters of latent variable models** with applications in clustering problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb035f0-243f-415b-a5f4-cd5f46569df5",
   "metadata": {},
   "source": [
    "### **A Probabalistic Modeling of Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf46f1-87b0-4e71-975f-ac7b96a286c8",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.0.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **Overview:** \n",
    "\n",
    "We will assume a joint distribution over both X (the data points), and Z (the labels). Each $x_i$ represents a point, and has a label, $z_i$, associated with it. Even though we don't observe the label from the actual observation, we assume there is a latent (hidden) label $z_i$ that we want to estimate. \n",
    "\n",
    "We also assume that both the labels and the data points are drawn from some joint distribution represented by$Pr(x, z \\mid\\theta)$, where $\\theta$ is some parameter. So, we must specify some family of distribution that generates both tha data points as well as the labels.\n",
    "\n",
    "Once we estimate the parameter using the posterior distribution, we can evaluate the probability of the label given each data point $x_i$ and the parameter we estimated.\n",
    "\n",
    "#### **Mathematical Breakdown**\n",
    "\n",
    "Using this example:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "We will start with the joint distribution of the data point and label, which can be written as $Pr(x, z)$. This can be broken down into two distributions, $Pr(z)$, and $Pr(x\\mid z)$. Thus the distributions we are looking at are:\n",
    "\n",
    "$$ Pr(x, z) = Pr(z)\\;Pr(x\\mid z) $$\n",
    "\n",
    "Then, we must specify how the label is generated. Since $z$ is one of two values, it can be defined as a discrete distribution. We can say that:\n",
    "\n",
    "$$ P(z = k) = \\pi_k $$\n",
    "\n",
    "Where $\\pi$ is the probability that a specific label is generated, and $\\pi_k \\ge 0, \\sum_k \\pi_k = 1$.\n",
    "\n",
    "Now that we have specified how the label is generated, we can specify how the data is generated given the label. It's very natural to assume that the distribution the data points are drawn from is a Gaussian distribution, as illustrated in the problem image above. Thus:\n",
    "\n",
    "$$ Pr(x\\mid z=k) = \\mathcal{N}(x\\mid \\mu_k \\sigma_k^2) $$\n",
    "\n",
    "Putting it all together and simplifying:\n",
    "\n",
    "$$ Pr(x\\mid z=k) = P(z = k)\\;Pr(x\\mid z=k) $$\n",
    "$$ = \\pi_k\\;\\mathcal{N}(x\\mid \\mu_k \\sigma_k^2) $$\n",
    "\n",
    "**This defines the joint distribution**.\n",
    "\n",
    "#### **Unknown Parameters**\n",
    "\n",
    "This distribution, $ = \\pi_k\\;\\mathcal{N}(x\\mid \\mu_k \\sigma_k^2) $, introduces some new parameters that we do not know beforehand, which we will call $\\theta$. These are the parameters that we do not observe directly:\n",
    "\n",
    "$$\\theta = \\{\\pi_k, \\mu_k, \\sigma_k\\}_{k=1}^K $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d17fea-d591-4bb6-bc63-8c708f9e861c",
   "metadata": {},
   "source": [
    "### **Estimating Theta in the Complete Information Case**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0d3221-0d40-4118-9b95-1a521b39b471",
   "metadata": {},
   "source": [
    "#### **Overview**\n",
    "The whole joint distribution, $Pr (x, z\\mid \\theta)$, depends on some parameter $\\theta$ that we do not directly observe. If we could estimate $\\theta$, including the $\\mu$, $\\sigma$, and $\\pi$, we will be able to infer what the label will be for a given $x$.\n",
    "\n",
    "**So how do we estimate theta?** Let's look at a simple case:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "\n",
    "It turns out that if we can observe $x_i$ and the corresponding label $z_i$ (called **complete information**), then we can estimate $\\theta$ by **maximizing the joint probability.**\n",
    "\n",
    "#### **Estimating Unknown Parameters Heuristically**\n",
    "\n",
    "$\\pi_k$ is easy to estimate. Since it is just the probability of the label being generated, it can be defined as:\n",
    "\n",
    "$$ \\pi_k = \\frac{\\#( z=k)}{n} $$\n",
    "$$ \\pi_1 = \\frac{1}{2} $$\n",
    "\n",
    "In this complete information case, $\\mu_1$ and $\\sigma^2_1$ can be estimated easily as well:\n",
    "\n",
    "$$\\mu_1 = \\frac{\\sum_{i=1}^n \\mathbb{I}(z_i=1)x_i}{\\sum_{i=1}^n \\mathbb{I}(z_i=1)} $$\n",
    "$$ \\sigma_1^2 = Var(\\{x_i \\mid z_i=1\\}) $$\n",
    "\n",
    "#### **Estimating Unknown Parameters Mathematically**\n",
    "\n",
    "We know that $Pr(x_i, z_i=k \\mid \\theta)$ is the joint probability for this problem, which we know we can solve by:\n",
    "\n",
    "$$ Pr(x_i, z_i=k \\mid \\theta) = \\pi_k\\;\\mathcal{N}(x\\mid \\mu_k \\sigma_k^2) $$\n",
    "\n",
    "And we can write the joint likelihood function as:\n",
    "\n",
    "$$ \\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{I}(z_i=k)\\;log(Pr(x_i, z_i=k \\mid \\theta)) $$\n",
    "\n",
    "Substituting in the joing probability calculation:\n",
    "\n",
    "$$ = \\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{I}(z_i=k)\\;log(\\pi_k\\;\\mathcal{N}(x\\mid \\mu_k \\sigma_k^2))$$\n",
    "\n",
    "Simplifying:\n",
    "\n",
    "$$ = \\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{I}(z_i=k)\\; (log(\\pi_k)+log(\\mathcal{N}(x_i\\mid \\mu_k \\sigma_k^2)))$$\n",
    "\n",
    "Which is the final joint likelihood function.\n",
    "\n",
    "#### **Maximizing the Unknown Parameters**\n",
    "\n",
    "Maximizing $\\pi_k$ using the above joint likelihood function:\n",
    "\n",
    "$$ \\pi_k = \\frac{\\sum_{i=1}^n\\mathbb{I}(z_i=k)}{n} $$\n",
    "\n",
    "Which matches our heuristic derivation. We will find that the same holds for $\\mu$ and $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d6434d-e1c6-495a-9944-f3d14b01aae1",
   "metadata": {},
   "source": [
    "### **Estimating Theta in the Incomplete Information Case**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69a8042-2d18-493c-9a1d-64ce3d04b3d0",
   "metadata": {},
   "source": [
    "#### **Overview**\n",
    "\n",
    "As we can see, estimating parameters in the complete information case is a fairly simple heuristic or MLE problem. However, in most real world cases, we will not observe the latent  variable $z$. We will **only observe the data points $\\{x_i\\}_{i=1}^n$**. This is called the **incomplete information** case, and in these cases we will estimate $\\theta$ by maximizing the **marginal probability**.\n",
    "\n",
    "We refer to $z_i$ as **missing information** in this case.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "We can still use MLE to estimate $\\theta$, except now the likelihood that we optimize should be defined on what we observe (in this case it will be defined on $x_i$). So in this case we want to maximize the likelihood of $x_i$:\n",
    "\n",
    "$$ \\sum_{i=1}^n log\\;Pr(x_i \\mid \\theta) $$\n",
    "\n",
    "Which is equal to the marginal probability of $x_i$, which is summing over all the possible $z_i$'s:\n",
    "\n",
    "$$ = \\sum_{i=1}^n log \\sum_{z_i} Pr(x_i, z_i \\mid \\theta) $$\n",
    "\n",
    "Unfortunately, this summation in the marginal probability makes the probability much more difficult, and there is no closed form solution for it. We must used numerical algorithms to solve the problem.\n",
    "\n",
    "#### **Why There is No Closed Form Solution**\n",
    "\n",
    "Defining the marginal distribution and likelihood for this problem:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Where the marginal likelihood is what we want to maximize with respect to $\\theta = \\{\\pi_k, \\mu_k, \\sigma_k\\}_{k=1}^K$. Since the summation from $k=1$ to $K$ falls within the log, there is no closed form solution to this problem.\n",
    "\n",
    "#### **Gaussian Mixture Models**\n",
    "\n",
    "This portion of the problem, which is a summation of Guassian probabilities, is a mixture of Gaussian distributions, making this a Gaussian Mixture problem:\n",
    "\n",
    "$$ Pr(x\\mid \\theta) = \\sum_k Pr(x, z=k \\mid \\theta) $$\n",
    "$$ = \\sum_k \\pi_k \\;\\mathcal{N}(x\\mid \\mu_k, \\sigma_k^2) $$\n",
    "\n",
    "And this is an example of a GMM:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.6.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834b723-f6d5-4cca-8f9e-b542b0c45144",
   "metadata": {},
   "source": [
    "### **Expectation Maximization and Maximizing the Gaussian Mixture Distribution**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c9607-7cd7-4806-ab04-262c3b0af491",
   "metadata": {},
   "source": [
    "#### **EM Background**\n",
    "While gradient descent may work, its not guaranteed to converge, we must determine step size, etc.. and there is a better way. **Expectation maximization** is an iterative algorithm similar to GD that allows us to maximize the marginal distribution much more conveniently. Recall that we want to maximize the marginal likelihood function, so really this is an optimization problem. \n",
    "\n",
    "#### **EM Algorithm**\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.7.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "The idea of the EM algorithm is that we start with some initialization, $\\theta_0$, some random guess. Then we iterate the following:\n",
    "\n",
    "- For each $\\theta_t$, we will impute the missing labels by using the posterior distribution:\n",
    "    - $z_i \\sim Pr(z\\mid x_i; \\theta_t)$\n",
    "- Then, update $\\theta$ by using the imputed $z_i$:\n",
    "    - $\\theta^{t+1} = \\underset{\\theta}{arg\\;max} \\sum_{i=1}^n \\mathbb{E}_{z_i\\sim Pr(\\cdot \\mid x_i ;\\theta_t)} [log\\;Pr(x_i,z_i\\mid \\theta)] $\n",
    " \n",
    "And these steps will be repeated until the algorithm converges.\n",
    "\n",
    "The advantage of this algorithms is that both of the steps of EM can be solved in closed form. Also, we can show that this procedure can montonically decrease the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558489af-2ab3-4c02-b623-73487c921f0e",
   "metadata": {},
   "source": [
    "### **Solving the Previous Example Using EM**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bc99f3-4881-46af-8d2c-0958655d126f",
   "metadata": {},
   "source": [
    "#### **Solving the Previous Example**\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.8.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Here, the unknown parameter $\\theta$ can be defined at iteration $t$ as $\\theta_t = \\{ \\pi_k^t, \\mu_k^t, \\sigma_k^t \\}$. \n",
    "\n",
    "**Step 1: Calculate the Posterior Distribution of $z_i=k$:**\n",
    "\n",
    "$$Pr(z_i=k\\mid x_i, \\theta_t) $$\n",
    "\n",
    "Using the rule of conditional probability (where $\\mathcal{l}$ is a dummy variable used to distinguish from $k$):\n",
    "\n",
    "$$ = \\frac{Pr(x_i, z_i=l \\mid \\theta_t}{\\sum_{\\mathcal{l}} Pr(x_i, z_i=l \\mid \\theta_t} $$\n",
    "\n",
    "Which, in this one-dimensional gaussian example, becomes:\n",
    "\n",
    "$$ = \\frac{\\pi_k \\;\\mathcal{N}(x_i\\mid \\mu_k^t, \\sigma_k^t)}{\\sum_l \\pi_l \\;\\mathcal{N}(x_i\\mid \\mu_l^t, \\sigma_l^t)} $$\n",
    "\n",
    "Explicitly, for $z_i=1$:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img width=\"60%\" src=\"images/2.4.9.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**Step 2: Maximize the Likelihood Function**\n",
    "\n",
    "Now, let's define gamma and the likelihood function as:\n",
    "- $\\gamma_{ik}^t = Pr(z_i=k\\mid x_i, \\theta_t)$ ($\\gamma_{ik}^t$ equals the posterior distribution at iteration $t$)\n",
    "- $L(\\theta) \\triangleq \\theta^{t+1} = \\underset{\\theta}{arg\\;max} \\sum_{i=1}^n \\mathbb{E}_{z_i\\sim Pr(\\cdot \\mid x_i ;\\theta_t)} [log\\;Pr(x_i,z_i\\mid \\theta)] $:\n",
    "\n",
    "$$ L(\\theta) = \\sum_{i=1}^n \\sum_{k=1}^K \\gamma_{ik}^t \\;log\\;Pr(x_i, z_i=k \\mid \\theta) $$\n",
    "\n",
    "This will actually be easy to solve, as we are just maximizing the joint likelihood of $x$ and $z$, except now we are averaging over gamma instead of the indicator function earlier.\n",
    "\n",
    "Without going into the derivation details:\n",
    "\n",
    "$$ \\pi_k^{t+1} = \\frac{\\sum_{i=1}^n \\gamma_{ik}^t}{n} $$\n",
    "$$\\mu_k^{t+1} = \\frac{\\sum_{i=1}^n \\gamma_{ik}^t x_i}{\\sum_{i=1}^n \\gamma_{ik}^t} $$\n",
    "$$\\sigma_k^{t+1} = \\dots $$\n",
    "\n",
    "This process is repeatedly iteratively, and is **guaranteed to converge to the local optimal solution**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {},
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
