{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "**outer product**: multiplying a column vector vs a row vector.\\\n",
    "**inner product**: multiplying a row vector by a column vector of the same dimension, resulting in a single scalar value.\\\n",
    "**orthogonal**:\\\n",
    "**principle axes**:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {},
   "source": [
    "## ***Introduction***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2ec31-1e8c-44c3-bb67-c6f2b821879c",
   "metadata": {},
   "source": [
    "### **Introduction**\n",
    "\n",
    "SVD is probably the most used and most important method for decomposing a matrix into a simpler structure, which helps you understand structural properties of the given matrix.\n",
    "\n",
    "### **Netflix Challenge Problem**\n",
    "\n",
    "This is equivalent to a \"matrix completion\" problem.\n",
    "\n",
    "**Imagine that you're Netflix, and you'd like to predict which users will like certain movies.**\n",
    "\n",
    "Consider a giant matrix, where the rows are people (Netflix subscribers), and the columns are movies. An entry in the matrix corresponds to the rating that person $i$ would give movie $j$. This is a real number, and higher numbers are better. Of course, this is a massive matrix. We also only know *some* of the entries, not all users watch or rate all movies. Netflix wants to take advantage of the information available to see if they can predict whether or not you'll like future movies. \n",
    "\n",
    "So he goal is to replace the missing entries with numbers that represent true preferences. In most cases, there is not much you can do to fill in empty values of a matrix. But in certain circumstances, with certain information, you can begin to fill in those entries.\n",
    "\n",
    "Consider the following matrix:\n",
    "\n",
    "$$ \\begin{pmatrix} 1 & ? & ? \\\\ ? & 2 & ? \\\\ ? & 6 & 9 \\\\ ? & ? & 3 \\\\ 4 & 4 & ? \\end{pmatrix} $$\n",
    "\n",
    "And the following additional information about the matrix: Each row is a multiple of the other rows.\n",
    "\n",
    "With that information, we can begin to deduce some of the values. For example, since the entry at [0,0] is a 1, and [4,0] is a 4, row 0 must be 1/4th of row 4. Thus, [0,1] is equal to 1.\n",
    "\n",
    "Using this fact we can fill in the entire matrix:\n",
    "\n",
    "$$ \\begin{pmatrix} 1 & 1 & \\frac{3}{2} \\\\ 2 & 2 & 3 \\\\ 6 & 6 & 9 \\\\ 2 & 2 & 3 \\\\ 4 & 4 & 6 \\end{pmatrix} $$\n",
    "\n",
    "### **Matrix Ranks**\n",
    "\n",
    "The extra information given to us to solve the previous matrix, \"Each row is a multiple of the other rows.\", is equivalent to saying \"The matrix has rank-1\".\n",
    "\n",
    "**Rank-0 Matrix**: The all 0's matrix\\\n",
    "**Rank-1 Matrix**: All rows are multiples of each other, or all columns are multiples of each other\n",
    "\n",
    "If we have a rank-1 $m \\times n$ matrix $A$, then\n",
    "\n",
    "$$ A = u \\otimes v^T $$\n",
    "\n",
    "Where $u$ is an $m \\times 1$ vector, and $v$ is an $n \\times1$ vector. (Note $\\otimes$ denotes an outer product operation).\n",
    "\n",
    "This means the $ij^{th}$ entry of $A \\equiv u_i * v_j$\n",
    "\n",
    "### **Matrix Notation**\n",
    "\n",
    "$$A = u \\otimes v^T \n",
    "= uv^T\n",
    "= \\begin{pmatrix} u_1*v^T \\\\ u_2*v^T \\\\ \\vdots \\\\ u_m*v^T \\end{pmatrix} \n",
    "= \\begin{pmatrix} u_1 v_1 & u_1 v_2 & \\dots & u_1 v_n \\\\ u_2 v_1 & u_2 v_2 & \\dots & u_2 v_n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ u_m v_1 & u_m v_2 & \\dots & u_m v_n \\end{pmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed5cd-43a9-4384-be36-0a59ee244e86",
   "metadata": {},
   "source": [
    "## ***Rank Matrices***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb82a9-1780-4c14-9406-a59c82ba1d90",
   "metadata": {},
   "source": [
    "Consider the case where $A$ is a rank-2 matrix. This means that $A$ is the sum of two rank-1 matrices (and $A$ is not rank-1). This means:\n",
    "\n",
    "$$A = uv^T + wz^T$$\n",
    "\n",
    "Thus, \n",
    "\n",
    "$$ A \n",
    "= \\begin{pmatrix} u_1 v^T + w_1 z^T \\\\ u_2 v^T + w_2 z^T\\\\ \\vdots \\\\ u_m v^T + w_m z^T\\end{pmatrix}\n",
    "= \\begin{pmatrix} u_1 v_1 + w_1 z_1 & u_1 v_2 + w_1 z_2 & \\dots & u_1 v_n + w_1 z_n \\\\ u_2 v_1 + w_2 z_1 & u_2 v_2 + w_2 z_2 & \\dots & u_2 v_n + w_2 z_n \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ u_m v_1 + w_m z_1 & u_m v_2 + w_m z_2 & \\dots & u_m v_n + w_m z_n \\end{pmatrix}$$\n",
    "\n",
    "This can also be thought of as multiplying an $m \\times 2$ matrix of columns $u$ and $w$ by a $2 \\times n$ matrix with rows $v^T$ and $^T$:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.12.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eefce1-6926-4e40-bb51-c850c23f3208",
   "metadata": {},
   "source": [
    "## ***Defining the Singular Value Decomposition***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47367318-9c44-41f9-acd7-2a9cdf4fb012",
   "metadata": {},
   "source": [
    "### **What is SVD**\n",
    "\n",
    "A matrix $A$ can be factorized as:\n",
    "\n",
    "$$A = USV^T$$ \n",
    "\n",
    "Where $U$ is an $m \\times n$ orthogonal matrix, and the columns of $U$ are the **left singular vectors**. $V$ is an $n \\times m$ orthogonal matrix, and the rows of $V^T$ are called the **right singluar vectors**. $S$ is an $m \\times n$ diagonal matrix. The entries of $S$ are the **singular values of $A$**, and can be ordered, $s_1 \\ge s_2 \\ge ... \\ge 0$, and are all greater than or equal to 0.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.12.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "There is only one set of singular values, and the singular values are unique. The singular vectors are not unique. \n",
    "\n",
    "Algebraically, $A$ can be written as:\n",
    "\n",
    "$$A = \\sum_{i=1}^{min(n,m)}s_iu_i \\otimes v_i^T $$\n",
    "\n",
    "### **What is SVD Doing**\n",
    "\n",
    "SVD essentially breaks down the matrix AA into three components:\n",
    "- Rotation/Reflection via $V^T$: The matrix $V^T$ rotates or reflects the space such that when applied to a vector, it aligns the directions with the principal axes of the matrix $A$.\n",
    "- Scaling via $S$: The diagonal matrix $S$ scales these directions by the singular values. The larger the singular value, the more important that direction is in capturing the behavior of AA.\n",
    "- Rotation/Reflection via $U$: Finally, $U$ applies another rotation or reflection to bring the space back into the original coordinates.\n",
    "\n",
    "### **Computational Complexity**\n",
    "\n",
    "The SVD can be computed in time $O(m^2n)$ or $O(n^2m)$, whicever is smaller.\n",
    "\n",
    "### **Summary**\n",
    "SVD decomposes a matrix into orthogonal directions (via $U$ and $V^T$) and scales those directions by singular values (via $S$). Itâ€™s a powerful tool for analyzing matrices, especially in dimensionality reduction, noise reduction, and understanding the underlying structure of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b538c2-528f-40e5-8724-e72f30fd0036",
   "metadata": {},
   "source": [
    "## ***Defining the Frobenius Norm***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3cf0c-981e-45cf-aa6b-ffa3d7783401",
   "metadata": {},
   "source": [
    "The Frobenius Norm of a matrix is:\n",
    "\n",
    "$$ \\sqrt{\\sum_{i|j}A_{ij}^2} $$\n",
    "\n",
    "Given a matrix $A$, we want to find a matrix $A'$ such that $A'$ has rank $k$ and minimuzed $||A-A'||_F$ over all rank $k$ matrices.\n",
    "\n",
    "How can we achieve this goal? Compute the SVD of $A$ and take the top $k$ singular vectors and values. Note that $A'$ is still size $m \\times n$.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.12.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f06ab-f3bb-472a-b95c-591d64b61762",
   "metadata": {},
   "source": [
    "## ***Matrix Completion***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421b457-b9cc-480e-9d5d-e1d610384235",
   "metadata": {},
   "source": [
    "### **Completing Matrices with Missing Values**\n",
    "\n",
    "What are the options to complete matrices with missing entries?\n",
    "\n",
    "There are some more naive options such as:\n",
    "- Input 0\n",
    "- Input the average of the known values\n",
    "- Input the average value in that column or row\n",
    "\n",
    "But the ideal choice is to:\n",
    "1. Fill in the unknown values with some guess \n",
    "1. Find the best rank $k$ approximation to $A$ after filling in the unknown values\n",
    "2. Output this best rank $k$ \n",
    "\n",
    "### **Intuition Behind This Approach**\n",
    "\n",
    "Even though we may have a giant matrix $A$ that we need to complete, if it is not totally unstructured and maybe has low rank, we can hope that there was enough information in the entries that we did observe that this best rank $k$ approximation is actually a good approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc478fde-bbf2-433e-af53-7f2f12ee47fd",
   "metadata": {},
   "source": [
    "## ***Choosing K***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c093a1bd-6d19-4c1c-b80f-ce2b3af34b9a",
   "metadata": {},
   "source": [
    "$k$ is a hyperparameter, so we must experiment with different values. One typical heuristic for choosing $k$ is to take enough singular values so that the sum of the remaining values is less than or equal to $\\frac{1}{10}$ of the values you did take."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c57489a-c788-47e1-ac97-1d49ac20ee44",
   "metadata": {},
   "source": [
    "## ***Applying SVD***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf60c09-ba84-488a-bf42-d8d723be4f56",
   "metadata": {},
   "source": [
    "### **Using SVD for Linear Regression**\n",
    "\n",
    "**SVD for the Linear Regression Problem when A is diagonal (Easy Case)**\n",
    "\n",
    "To illustrate how SVD works, let's consider the linear regression problem:\n",
    "\n",
    "$$ \\underset{x \\in \\mathbf{R}^n}{\\min} ||Ax-b||^2 $$\n",
    "\n",
    "Where $ b \\in \\mathbf{R}^m$, and $A$ is an $m\\times n$ matrix. The easy case is that $A = D$ (diagonal):\n",
    "\n",
    "$$ D = \\begin{pmatrix} d_1 & & & \\\\ & d_2 & & \\\\ & & \\ddots & \\\\ & & & 0 \\end{pmatrix} $$\n",
    "\n",
    "**What is the optimal value for $x_1$?**\n",
    "\n",
    "$x_1 = \\frac{b_1}{d_1}$, $x_2 = \\frac{b_2}{d_2}$, etc. \\\n",
    "If $d_j = 0 \\implies x_j = 0$.\n",
    "\n",
    "So the solution is to multiply $b$ times $D^\\dagger$ (the pseudoinverse of $D$), where:\n",
    "$$D^\\dagger = \\begin{pmatrix} \\frac{1}{d_1} & & & \\\\ & \\frac{1}{d_2} & & \\\\ & & \\ddots & \\\\ & & & 0 \\end{pmatrix} $$\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "$$ x = D^\\dagger * b $$\n",
    "\n",
    "### **Solving A in General**\n",
    "\n",
    "Now let's solve for $A$ when $A$ is not necessarily diagonal.\n",
    "\n",
    "$$ ||Ax-b||^2 \\equiv \\underset{x}{\\min} ||USV^Tx-b||^2$$\n",
    "$$ ||Ux|| = ||x|| \\equiv \\underset{x}{\\min} ||SV^Tx-U^Tb||^2$$\n",
    "$$ y = V^Tx \\equiv Vy = x $$\n",
    "$$ \\equiv \\underset{x}{\\min} ||Sy-U^Tb||^2 $$\n",
    "$$ \\implies y = S^\\dagger*U^Tb $$\n",
    "$$ \\implies x = VS^\\dagger U^Tb $$\n",
    "\n",
    "So, for the linear regression problem:\n",
    "\n",
    "$$ x = VS^\\dagger U^Tb $$\n",
    "\n",
    "### **Using SVD to Perform PCA**\n",
    "\n",
    "Recall in PCA we wanted to find the eigendecomposition of a covariance matrix. Starting with $X^TX$ and taking the SVD of it:\n",
    "\n",
    "$$X^TX = (USV^T)^T*USV^T$$\n",
    "\n",
    "Transposing the left matrix gives:\n",
    "\n",
    "$$X^TX = VSU^T*U*SV^T$$\n",
    "\n",
    "Since $U^TU$ is the identity matrix:\n",
    "\n",
    "$$X^TX = VS^2V^T$$\n",
    "\n",
    "#### **Information Gained From SVD**\n",
    "\n",
    "Since $V$ is orthogonal and $S$ is diagonal, using SVD on $X^TX$ has yielded the eigendecomposition of $X^TX$.\n",
    "\n",
    "And, the right singular vectors of $X$ (rows of $V^T$) **are** the principle components (top eigenvectors of $X^TX$). In this way, SVD provides more information that PCA.\n",
    "\n",
    "And, the singular values are the square root of the eigenvalues of $X^TX$.\n",
    "\n",
    "### **Using SVD to Perform Image Compression**\n",
    "\n",
    "See the paper \"A Singularly Valuable Decomposition: The SVD of a Matrix\".\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.12.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
