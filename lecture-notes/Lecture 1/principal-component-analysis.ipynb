{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# Principal Component Analysis #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "**Covariance Matrix**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***1.11.0 Introduction***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2ec31-1e8c-44c3-bb67-c6f2b821879c",
   "metadata": {},
   "source": [
    "#### **Introduction**\n",
    "\n",
    "PCA is possibly the most important technique for reducing dimensionality of data.\n",
    "\n",
    "Introductory details:\n",
    "- Dimensionality reduction technique\n",
    "- PCA works for any $k$\n",
    "- PCA looks at the data to find a new representation, meaning it is data-dependent\n",
    "\n",
    "Preview of the PCA process:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.10.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **High-level goal of PCA**\n",
    "\n",
    "The goal of PCA is to find vectors $v_1, ..., v_k$ such that:\n",
    "\n",
    "$$ \\forall x \\in S\\;\\;\\;\\; x\\approx \\sum_{j=1}^k a_jv_j$$\n",
    "\n",
    "#### **A Note About Data Preprocessing**\n",
    "\n",
    "For the dataset $S$, we must perform the following preprocessing:\n",
    "- subtract the mean or center of mass from each data point (the mean will now be 0)\n",
    "- normalize by the standard deviation of each feature (we want the features to have relatively similar values in features)\n",
    "    - For every feature in $i$ perform:\n",
    " $$ \\sqrt{\\frac{1}{m}\\sum_{j=1}^m(x_i^j)^2} = \\sigma_i$$\n",
    "    - Then divide all the $i^th$ features by $\\sigma_i$\n",
    "- We want each feature to, as closely as possible, have mean 0 and standard deviation 1\n",
    "\n",
    "#### **How to Begin**\n",
    "Find $v_1$, a vector that minimizes square-distance. So,\n",
    "\n",
    "$$ \\underset{v, \\;||v||_2 \\;= 1}{\\min}\\;\\;\\;\\frac{1}{m}\\sum_{j=1}^m(distance\\;between\\;x^j\\;and\\;v)^2 $$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "We want to find the direction, that when we project all of the points orthogonally down that direction, the sum of the squares of the distances is minimal. The difference from linear regression is that for the loss we are measuring the distance from each point via its orthogonal projection onto the line, and then take squares."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed5cd-43a9-4384-be36-0a59ee244e86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***1.11.1 Explaining PCA***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb82a9-1780-4c14-9406-a59c82ba1d90",
   "metadata": {},
   "source": [
    "#### **Objective**\n",
    "\n",
    "Recall our objective function which we wish to minimize:\n",
    "$$ \\underset{v, \\;||v||_2 \\;= 1}{\\min}\\;\\;\\;\\frac{1}{m}\\sum_{j=1}^m(distance\\;between\\;x^j\\;and\\;v)^2 $$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Because of the pythagorean theorem, as illustrated above, since $||x||_2^2$ is fixed, we can have an equivalent objective formulation, which is to find a $v$ such that it maximizes the distance from $x$ to $v$:\n",
    "\n",
    "$$ \\underset{v, \\;||v||_2 \\;= 1}{\\max}\\;\\;\\;\\frac{1}{m}\\sum_{j=1}^m\\langle x^i, v \\rangle ^2 $$\n",
    "\n",
    "This is referred to as **the direction of maximal variance**. This value is equal to the sample variance of our data in the direction $v$ (recall that $var = \\mathbf{E}[x^2] - (\\mathbf{E}[x])^2$, where $(\\mathbf{E}[x])^2 = 0$ since we subtracted out the mean from each point).\n",
    "\n",
    "#### **Main Idea**\n",
    "\n",
    "The main goal is to find the vector, or vectors that retains the most variance from the dataset. In the below example, $v_2$ is the better vector because it preserves more of the variance of the points.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "***Question: That was a good example for 1 vector, but what about $k$ vectors/components?*** \n",
    "\n",
    "The optimization problem for this is:\n",
    "\n",
    "$$ \\underset{s\\;of\\;dimension\\;k}{max\\;subspaces}\\;\\;\\;\\frac{1}{m}\\sum_{j=1}^m (length\\;of\\;x^j\\;projected\\;onto\\;S)^2 $$\n",
    "\n",
    "A nice and prefereable basis would be an orthonormal basis $v_1, ..., v_k$. This is because if we want to understand the length of some point $x$ projected onto $S$, and we have an orthonormal basis, then we can just project the point $x$ onto each vector in the basis and take the sum of the squares directly.\n",
    "\n",
    "Assuming that we have an orthonormal basis:\n",
    "\n",
    "$$ (distance\\;from\\;x^j\\;to\\;S)^2 = ||x||^2-(\\langle x,v_1,\\rangle^2 + ... + \\langle x,v_k,\\rangle^2) $$\n",
    "\n",
    "With all of this context...\n",
    "\n",
    "#### **The Formal PCA Objective**\n",
    "\n",
    "$$ \\underset{v_1,...,v_k\\;;\\;orthogonal}{max}\\;\\;\\;\\frac{1}{m}\\sum_{j=1}^m \\sum_{i=1}^k\\langle x^j,v_i\\rangle ^2 $$\n",
    "\n",
    "***Question: Let's assume we have $v_1, ..., v_k$... How do we express $x$ once we have these vectors?***\n",
    "\n",
    "We can find this by simply taking its projection onto each basis vector: $$x = \\langle x, v_1\\rangle * v_1 + \\langle x, v_2\\rangle * v_2 + ... + \\langle x, v_k\\rangle * v_k$$\n",
    "\n",
    "As such, $x$ can be written as a vector in $\\mathbf{R}^k$ corresponding to these projections."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eefce1-6926-4e40-bb51-c850c23f3208",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***1.11.2 Applications***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47367318-9c44-41f9-acd7-2a9cdf4fb012",
   "metadata": {},
   "source": [
    "#### **Applications of PCA**\n",
    "\n",
    "**1. Understanding genomes - \"Genes Mirror Geography Within Europe\"**\n",
    "\n",
    "A researcher took 1,400 people from Europe, and each person was represented according to about 200,000 genetic markers in their genome. So, each person was represented purely by genetic material, and the number of features was $\\approx$ 200,000.\n",
    "\n",
    "This corresponds to a matrix of dimension 1400 x 200,000. The researchers ran PCA on this data to find vectors $v_1$ and $v_2$. So now, each person corresponds to two numbers. Then the researchers plotted the two numbers, and color coded each point according to the country of origin. The plot is below, and as you can see, it is almost exactly a map of Europe rotated 16 degrees.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**2. Image Data Compression - \"Eigenfaces\"**\n",
    "\n",
    "This is a strategy for compressing image data, where the images are of faces. Each data point is an image (vector of pixels), and each image has 65,000 pixels. So that means we have 65,000 features, with each feature being a pixel. \n",
    "\n",
    "Researchers ran PCA on this dataset with $k \\approx 100 \\;to\\;150$. Now each image $\\approx$ linear combination of 150 vectors of length 65,000.\n",
    "\n",
    "Below is one of the original images which was a vector of length 65,000, next to the compressed version of a vector of length $\\approx$ 150.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.6.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b538c2-528f-40e5-8724-e72f30fd0036",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***1.11.3 The Big Question***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3cf0c-981e-45cf-aa6b-ffa3d7783401",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **How do we find these vectors v1 through vk?**\n",
    "\n",
    "Recall the optimization problem:\n",
    "\n",
    "$$ \\underset{v_1,...,v_k\\;;\\;orthonormal}{max}\\;\\;\\;\\frac{1}{m}\\sum_{j=1}^m \\sum_{i=1}^k\\langle x^j,v_i\\rangle ^2 $$\n",
    "\n",
    "### **Setup**\n",
    "- Let $X$ be an $m$ by $n$ matrix \n",
    "- $m$ is the number of points in our training set \n",
    "- $n$ is the dimension.\n",
    "- $v$ denotes a column vector \n",
    "- $v^T$ denotes a row vector \n",
    "- $v^Tv$ is an inner product (scalar) \n",
    "- $vv^T$ is an outer product (matrix)\n",
    "\n",
    "We will now look at $X^TX$, which is an $n$ by $n$ matrix. Multiplying this matrix by $\\frac{1}{m}$ will yield the **sample covariance matrix**, so $\\frac{1}{m}X^TX$. The $(i,j)^m$ entry of $X^TX$ corresponds to \"how simiar is feature $i$ to feature $j$\".\n",
    "\n",
    "---\n",
    "\n",
    "**Notes:**\n",
    "- $X^TX$ is a symmetric matrix.\n",
    "- All eigenvalues of symmetric matrices are $\\ge 0$.\n",
    "- For a matrix $A$, $v$ is an eigenvector if $A*v = \\lambda*v$ for some $\\lambda \\in \\mathbf{R}$, where $\\lambda$ is an eigenvalue.\n",
    "- An orthogonal matrix is one where all the columns are orthonormal $\\iff A^TA = I$. Therefore also $AA^T = I$.\n",
    "\n",
    "**Spectral Theorem:** \\\n",
    "Every symmetric matrix $A$ has an eigendecomposition: $$A=Q*D*Q^T$$ Where $Q$ is an orthogonal matrix, and $D$ is a diagonal matrix. The entries of $D$ are the eigenvalues of $A$.\n",
    "\n",
    "---\n",
    "\n",
    "Let's try to compute $v_1$ now. Recall $X$ is the matrix corresponding to $S$, so $X$ is $m$ by $n$. If we multiple $X$ by a vector $v$, we get, where each $x_i$ is a row of $X$:\n",
    "\n",
    "$$   \\begin{align}\n",
    "    X\\cdot v &= \\begin{bmatrix}\n",
    "           \\langle x_{1}, v \\rangle \\\\\n",
    "           \\vdots \\\\\n",
    "           \\langle x_{m}, v \\rangle\n",
    "         \\end{bmatrix}\n",
    "  \\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "If we take the inner product with itself:\n",
    "\n",
    "$$ (Xv)^T \\cdot (Xv) = \\sum_{i=1}^m \\langle x_i, v \\rangle ^ 2 $$\n",
    "\n",
    "Which can also be written as:\n",
    "\n",
    "$$ v^TX^TXv = (Xv)^T \\cdot (Xv) = \\sum_{i=1}^m \\langle x_i, v \\rangle ^ 2 $$\n",
    "\n",
    "Notice the similarity to the PCA objective function.\n",
    "\n",
    "Recall we are trying to find a $v$ that maximizes $\\langle x_i, v \\rangle ^2$, thus equivalently (as shown above), we want to find a vector $v$ that maximizes $v^T(X^TX)v$. We will call $(X^TX)$ $A$. This is called \"maximizing a quadratic form\".\n",
    "\n",
    "With this notation, our new goal is:\n",
    "\n",
    "$$ \\underset{v, \\;||v||_2 \\;= 1}{\\max}\\;\\;\\;v^TAv $$\n",
    "\n",
    "---\n",
    "\n",
    "Let's look at a simple case, $A$ is diagonal.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.7.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "***Question: Which $v$ should we choose?***\n",
    "\n",
    "We should pick $v = (1, ..., 0)$. Why? $v$ has to have norm 1, and $A$ is constructed such that $\\lambda_1$ is the largest eigenvalue. So we should pick $v$ that picks off that largest eigenvalue.\n",
    "\n",
    "Another way to view $v$:\n",
    "\n",
    "$$ v^TAv = v_1, ..., v_n\\cdot\n",
    "    \\begin{align}\n",
    "        \\begin{bmatrix} \n",
    "            \\lambda_1 \\cdot v_1\\\\ \n",
    "            \\vdots \\\\ \n",
    "            \\lambda_n \\cdot v_n \n",
    "        \\end{bmatrix} \n",
    "    \\end{align}\n",
    " = \\sum_{i=1}^n v_i^2 \\cdot  \\lambda_i\n",
    "$$\n",
    "\n",
    "So whatever $v$ you choose will be the sum of $v_i^2 \\cdot \\lambda_i$\n",
    "\n",
    "---\n",
    "\n",
    "**However, we don't know if $A$ is diagonal**, but we do know that $A = Q*DQ^T$ where $D$ *is* diagonal. So $A$ is *almost* diagonal. Thus, instead of picking a vector that has a 1 in the first component and 0 everywhere else, the correct thing to do is...\n",
    "\n",
    "Let $e_1 = (1, 0, ..., 0, 0)$, the matrix with a 1 in the first component and 0 everywhere else.\n",
    "\n",
    "Choose $v = Q*e_1$. This is the vector that maximizes the objective function. This is the top eigenvector of $A$.\n",
    "\n",
    "### **Breakdown**\n",
    "\n",
    "**Step 1: Covariance Matrix $A$**\n",
    "\n",
    "We start with a covariance matrix $A$:\n",
    "\n",
    "$$\n",
    "A = \\begin{pmatrix} 3 & 1 \\\\ 1 & 2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Step 2: Compute Eigenvalues and Eigenvectors**\n",
    "\n",
    "To find the eigenvalues ($\\lambda$) and eigenvectors ($v$) of $A$, we solve the characteristic equation:\n",
    "\n",
    "$$\n",
    "\\text{det}(A - \\lambda I) = 0\n",
    "$$\n",
    "\n",
    "This results in the eigenvalues:\n",
    "\n",
    "$$\n",
    "\\lambda_1 = 3.618 \\quad \\text{and} \\quad \\lambda_2 = 1.382\n",
    "$$\n",
    "\n",
    "Next, for each eigenvalue $\\lambda_i$, we solve the equation:\n",
    "\n",
    "$$\n",
    "(A - \\lambda_i I)v_i = 0\n",
    "$$\n",
    "\n",
    "For $\\lambda_1 = 3.618$:\n",
    "\n",
    "$$\n",
    "(A - 3.618I)v_1 = 0 \\implies \\begin{pmatrix} -0.618 & 1 \\\\ 1 & -1.618 \\end{pmatrix} v_1 = 0\n",
    "$$\n",
    "\n",
    "The solution yields the eigenvector:\n",
    "\n",
    "$$\n",
    "v_1 = \\begin{pmatrix} 0.850 \\\\ 0.526 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "For $\\lambda_2 = 1.382$:\n",
    "\n",
    "$$\n",
    "(A - 1.382I)v_2 = 0 \\implies \\begin{pmatrix} 1.618 & 1 \\\\ 1 & 0.618 \\end{pmatrix} v_2 = 0\n",
    "$$\n",
    "\n",
    "The solution yields the eigenvector:\n",
    "\n",
    "$$\n",
    "v_2 = \\begin{pmatrix} -0.526 \\\\ 0.850 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Step 3: Diagonalization of $A$**\n",
    "\n",
    "Using the eigenvectors $v_1$ and $v_2$, we can form the matrix $Q$:\n",
    "\n",
    "$$\n",
    "Q = \\begin{pmatrix} 0.850 & -0.526 \\\\ 0.526 & 0.850 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "The diagonal matrix $\\Lambda$ (which contains the eigenvalues) is:\n",
    "\n",
    "$$\n",
    "\\Lambda = \\begin{pmatrix} 3.618 & 0 \\\\ 0 & 1.382 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "So, the diagonalization of $A$ is given by:\n",
    "\n",
    "$$\n",
    "A = Q \\Lambda Q^T\n",
    "$$\n",
    "\n",
    "**Step 4: Maximize the Objective Function**\n",
    "\n",
    "The objective function in PCA is to maximize the variance of the data projected onto $v$. This is done by maximizing $v^T A v$, where $v$ is a unit vector.\n",
    "\n",
    "$$\n",
    "v^T A v = v^T Q \\Lambda Q^T v\n",
    "$$\n",
    "\n",
    "Substituting $u = Q^T v$ (where $u$ is also a unit vector):\n",
    "\n",
    "$$\n",
    "v^T A v = u^T \\Lambda u = \\sum_{i=1}^n \\lambda_i u_i^2\n",
    "$$\n",
    "\n",
    "To maximize $v^T A v$, we set $u = e_1 = (1, 0, \\dots, 0)$ to align with the largest eigenvalue $\\lambda_1$:\n",
    "\n",
    "$$\n",
    "v = Q e_1 = \\begin{pmatrix} 0.850 \\\\ 0.526 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "**Step 5: Interpretation**\n",
    "\n",
    "Choosing $v = Q e_1$ means selecting the eigenvector $v_1$ corresponding to the largest eigenvalue $\\lambda_1$. This vector $v_1$ is the direction that maximizes the variance of the data when projected onto it, which is the principal component in PCA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08f06ab-f3bb-472a-b95c-591d64b61762",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***1.11.4 Recap***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a421b457-b9cc-480e-9d5d-e1d610384235",
   "metadata": {},
   "source": [
    "### **Example Optimization Problem**\n",
    "\n",
    "Our optimization problem is:\n",
    "\n",
    "$$ \\underset{v, \\;||v||_2 \\;= 1}{\\max}\\;\\;\\;v^TAv $$\n",
    "\n",
    "Where $A$ corresponds to a covariance matrix $X^TX$ (some people use $\\frac{1}{m}X^TX$ but we will not be).\n",
    "\n",
    "And the \"easy case\" was when $A$ was a diagonal matrix. For example, let \n",
    "\n",
    "$$ A = \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} $$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.8.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "The visualization shows how the chosen vector will capture the axis that keeps the most information about the data.\n",
    "\n",
    "### **Example Solution to Optimization Problem**\n",
    "\n",
    "The solution to\n",
    "\n",
    "$$ \\underset{v, \\;||v||_2 \\;= 1}{\\max}\\;\\;\\; v^T\\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix}v $$\n",
    "\n",
    "is $v = (1, 0)$, which corresponds precisely to the stretching on the horizontal axis shown above. This is the direction of maximum variance.\n",
    "\n",
    "### **Rotation Matrices**\n",
    "\n",
    "A rotation matrix is an orthogonal matrix that rotate your coordinate axis. \n",
    "\n",
    "This matrix will rotate the axes $\\theta$ degrees counterclockwise:\n",
    "\n",
    "$$ \\begin{pmatrix} cos\\theta & -sin\\theta \\\\ sin\\theta & cos\\theta \\end{pmatrix} $$\n",
    "\n",
    "This matrix will rotate the axes $\\theta$ degrees clockwise:\n",
    "\n",
    "$$ \\begin{pmatrix} cos\\theta & sin\\theta \\\\ -sin\\theta & cos\\theta \\end{pmatrix} $$\n",
    "\n",
    "### **Covariance Matrix Rotation**\n",
    "\n",
    "For any matrix of the form $X^TX$ (a covariance matrix), you can write that covariance matrix as a diagonal matrix times some rotation matrices.\n",
    "\n",
    "For example:\n",
    "\n",
    "$$ \\begin{pmatrix} 2 & 1 \\\\ 1 & 2 \\end{pmatrix} = \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & -\\frac{1}{\\sqrt{2}} \\\\ \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix} \\cdot \\begin{pmatrix} 3 & 0 \\\\ 0 & 1 \\end{pmatrix} \\cdot \\begin{pmatrix} \\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\\\ -\\frac{1}{\\sqrt{2}} & \\frac{1}{\\sqrt{2}} \\end{pmatrix}$$\n",
    "\n",
    "Recall that $\\frac{1}{\\sqrt{2}}$ is the $cos$ of 45 degrees. So this is a rotation 45 degrees clockwise.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.9.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **Online Resources**\n",
    "\n",
    "Online matrix calculators, such as BlueBit, can be used for matrix operations including eigenvector decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc478fde-bbf2-433e-af53-7f2f12ee47fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## ***1.11.5 Spectral Theorem***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3994a4-4bda-425d-908c-26938ba1489d",
   "metadata": {},
   "source": [
    "#### **Spectral Theorem Definition**\n",
    "\n",
    "Any symmetric matrix can be written: $$A=Q*D*Q^T$$ Where $Q$ is an orthogonal matrix, and $D$ is a diagonal matrix with real values on the diagonal. The entries of $D$ are the eigenvalues of $A$.\n",
    "\n",
    "Furthermore, if $A = X^TX$, then all eigenvalues $\\ge 0$.\n",
    "\n",
    "#### **Proof**\n",
    "\n",
    "**Claim 1: For any $v$, $v^TAv \\ge 0$.**\n",
    "\n",
    "This is proven by $A = X^TX$, $v^TAv = (Xv)^T \\cdot Xv \\ge 0$.\n",
    "\n",
    "**Claim 2: $A$ cannot have negative eigenvalues**\n",
    "\n",
    "Let's assume by contradiction that $\\lambda_i \\lt 0$ ($i^th$ eigenvalue is negative).\n",
    "\n",
    "Rewriting $A$ by the Spectral Theorem, $A = Q*D*Q^T$. Let's now consider the vector $Q*e_i$, where $e_i = (0, 0, ..., 1, 0, ..., 0)$ where the 1 is in the $i^th$ position in the vector. We now call $v = Q*e_i$.\n",
    "\n",
    "Consider now $v^TAv$, by substitution this is equivalent to $e_i^TQ^TQDQ^TQe_i$. Since $Q^TQ = I$, we are left with $e_i^TDe_i$. Since we assumed by contradiction that D has a negative eigenvalue, $D*e_i$ is a vector of all 0's with a negative value in the $i^th$ position. Thus, $e_i^TDe_i \\lt 0$.\n",
    "\n",
    "However, this contradicts claim 1, and therefore is false.\n",
    "\n",
    "---\n",
    "\n",
    "This theorem and properties are why we can view or interpret covariance matrices geometrically, as rotations and scalings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b514e2e-3ed3-466b-bc0c-be5be7a003df",
   "metadata": {},
   "source": [
    "## ***1.11.6 Eigenvector Decomposition***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c093a1bd-6d19-4c1c-b80f-ce2b3af34b9a",
   "metadata": {},
   "source": [
    "To recap the process of PCA:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.10.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Proving that the $i^th$ row of $Q^T$ is an eigenvector of $A$:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.11.11.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "***Question: What algorithm should we use to compute this decomposition?***\n",
    "\n",
    "We must deal with the **\"singular value decomposition\"** (SVD) of the matrix. If this is computed, then you will get the eigenvector eigenvalue decomposition of the matrix. There exist polynomial time algorithms for computing the SVD, but are expensive and we would like to avoid them for large datasets. So, instead the method used in practice is the power method [lecture abruptly cuts off and ends lol].\n",
    "\n",
    "The next lecture covers SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": [
    "none yet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
