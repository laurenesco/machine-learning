{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fcaa1f5-813b-4a2f-a9fa-59e2c99648a7",
   "metadata": {},
   "source": [
    "# Logistic Regression #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae6e90c-a17e-4487-81f4-af052f0a5288",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3742a8-a70b-4aca-8c08-39b093c51b74",
   "metadata": {},
   "source": [
    "none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea1138-c3c9-4647-8a46-cc3446e57c0b",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d2aca6-5d77-4737-a743-5b7f429fbde2",
   "metadata": {},
   "source": [
    "## ***1.10.0 Introduction***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9eddbad-b8e4-4e76-8164-39c8c08a9e75",
   "metadata": {},
   "source": [
    "### **Loss Functions**\n",
    "- Classification\n",
    "    - loss function: we predict $sign(w^Tx)$ for some vector $w$, and we will be penalized if $sign(w^Tx) \\ne y^i$, and there will be no penalty if $sign(w^Tx) = y^i$. We examine $y^i*(w^Tx^i)$, and if positive, no penalty, if negative we enact penalty.\n",
    "    - Penalty can be 0-1 loss: $\\phi_{0-1}(y^i*w^Tx^i)$, where\n",
    "$$ \\phi_{0-1}(z) = \\begin{cases} \n",
    "          1\\;if\\;z\\;\\le\\;0 \\\\\n",
    "          0\\;if\\;z\\;\\gt\\;0 \n",
    "          \\end{cases}\n",
    "$$  \n",
    "- Linear Regression\n",
    "    - loss function (square loss): $\\underset{w}{\\min}$ is $\\frac{1}{m} \\sum_{i=1}^{m}(w^Tx^i-y^i)^2$ was our loss objective\n",
    "    - Square loss penalty: least squared loss $(w^Tx^i-y^i)^2$\n",
    "    - We knew that if $\\mathbf{E}[Y|X] = w^Tx$, then linear regression is a great option. Even if it did not hold, we could use GD to find the best fitting line\n",
    "\n",
    "### **Optimization Problems**\n",
    "- Classification\n",
    "    - $\\underset{w}{\\min} \\frac{1}{m} \\sum_{i=1}^{m}\\phi_{0-1}(y^i*w^Tx^i)$, this is what we want to solve for classification.\n",
    "\n",
    "---\n",
    "\n",
    "***Question: When does perceptron find a $w$ with small loss?***\n",
    "<br><br>\n",
    "Recall that perceptron required $\\exists \\;w \\;such\\;that\\; \\forall x: y*w^Tx > \\rho$. There had to be a $w$ that for $w^Tx$ was negative, it was correct, and the margin was at least $\\rho$. This implies convergence, or a number of mistakes, at most $\\frac{1}{\\rho^2}$.\n",
    "\n",
    "<br>\n",
    "\n",
    "***Question: What if there is no margin? (There may not even be a $w$ that is consistent with all the labels in our trianing set!)*** \n",
    "<br><br>\n",
    "We can still use $\\underset{w}{\\min} \\frac{1}{m} \\sum_{i=1}^{m}\\phi_{0-1}(y^i*w^Tx^i)$, becuase it will still minimize the number of mistakes on our training set. The goal in this case is to find a linear function the separates the data while making as few mistakes as possible.\n",
    "\n",
    "<br> \n",
    "\n",
    "What is that computational complexity of this optimization problem?\n",
    "\n",
    "$$\\underset{w}{\\min} \\frac{1}{m} \\sum_{i=1}^{m}\\phi_{0-1}(y^i*w^Tx^i)$$\n",
    "\n",
    "This problem is NP-hard, it is unlikely to admit a polynomial time solution. It is not convex nor differentiable, as was the linear regression optimization problem. This problem is sometimes referred to as \"agnostically learning a halfspace\". (There is a model of learning called agnostic learning, which is generalized PAC learning to a noisy halfspace).\n",
    "\n",
    "Because of this, we aim to find a loss function for classification that is more relaxed. That will be logistic loss, which will turn out to be convex and differentiable.\n",
    "\n",
    "---\n",
    "\n",
    "### **TLDR;**\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.10.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "We will be looking at relaxing the 0-1 loss function to create logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10e005-02b7-42fd-a2a4-bf25ef0ae6e5",
   "metadata": {},
   "source": [
    "## ***1.10.1 Losses***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f09c6d-c339-4e96-9e44-6f709c5fedd9",
   "metadata": {},
   "source": [
    "#### **Introducing some new losses**\n",
    "- $\\phi_{logistic}$\n",
    "- $\\phi_{hinge}$\n",
    "- $\\phi_{exp}$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Logistic Loss Function**\n",
    "\n",
    "$$ \\phi_{logistic}(z) = log(1+e^{-z}) $$\n",
    "\n",
    "Notice if we plug in our values:\n",
    "\n",
    "$$ \\phi_{logistic}(y^i*w^Tx^i) = log(1+e^{-(y^i*w^Tx^i)}) $$\n",
    "\n",
    "If $(y^i*w^Tx^i)$, which is often referred to as ***margin***, is $<< 0$, then $w^Tx^i$ has a different sign than $y^i$, meaning the guess was incorrect. This makes $ \\phi_{logistic}(y^i*w^Tx^i)$ large, because of the negation. Likewise, if the guess was correct, then $ \\phi_{logistic}(y^i*w^Tx^i)$ is small, moving to 0.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.10.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Hinge Loss Function**\n",
    "\n",
    "$$ \\phi_{hinge}(z) = max\\{1-z,\\;0\\} $$\n",
    "\n",
    "Notice if we plug in our values:\n",
    "\n",
    "$$ \\phi_{hinge}(y^i*w^Tx^i) = max\\{1-(y^i*w^Tx^i),\\;0\\} $$\n",
    "\n",
    "When our prediction is correct by a margin of 1 or greater, there will be no loss. When our prediction is incorrect, there will be loss. Notice in this loss function that even if we are correct but the margin is .5, we will still incur some loss.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.10.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Exponential Loss Function**\n",
    "\n",
    "$$ \\phi_{exp}(z) = e^{-z} $$\n",
    "\n",
    "Which has similar properties.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Visualizing the Losses**\n",
    "Visualizing these losses:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.10.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Notice the are all convex and differentiable. \n",
    "\n",
    "We will be focusing on logistic loss for the rest of the notes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78a2d3c-5748-4a9b-bb02-d8aa700de667",
   "metadata": {},
   "source": [
    "## ***1.10.2 Logistic Loss Optimization***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ea4c37-057a-4821-9c68-dc4c6c81fbe2",
   "metadata": {},
   "source": [
    "#### **Optimization Problem Associated with Logistic Loss:**\n",
    "\n",
    "$$L(w) = \\frac{1}{m} sum_{i=1}^{m}log(1+exp(-y^i*w^Tx^i))$$\n",
    "\n",
    "so we want to find $\\underset{w}{\\min}\\;L(w)$. Enter the **sigmoid function**:\n",
    "\n",
    "$$ g(z) = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Notice that\n",
    "- as $z$ gets larger, $g(z) \\to 1$\n",
    "- as $z$ gets smaller, $g(z) \\to 0$.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.10.5.png\" alt=\"Professor Notes\" style=\"width: 50%;\"/>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **Properties of sigmoid**\n",
    "Fact, for the sigmoid function, $ g(z) = \\frac{1}{1+e^{-z}}$: $$g(z) + g(-z) = 1$$\n",
    "\n",
    "Also, $\\exists \\;w$ such that $$\\mathbf{E}[Y|X] = g(Y*w^TX)$$\n",
    "\n",
    "Which means that $$ \\implies Pr[Y=1|X] = g(w^Tx)$$\n",
    "\n",
    "Thus, given $x$, if $w^Tx$ is large, the probability that $y$ equals 1 is very large. Likewaise, if $w^Tx$ is negative and small, the probability that $y$ equals 1 is very small. This is due to the shape of the sigmoid function, and this is the relationship between the sigmoid function and the halfspace scenario.\n",
    "\n",
    "#### **Relation to Halfspaces**\n",
    "If your $w^Tx$ was large in the halfspace scenario, your label was definitely one, because it was positive. In logistic regression, we are going to assume that the probability that it will equal 1 is very large, but there is still some chance it could still equal zero (and likewise for a small $w^Tx$. Hence the relaxation.\n",
    "\n",
    "In the sigmoid function, if $w^Tx = 0$, you will be equally likely to have a label 1 or 0.\n",
    "\n",
    "#### **Model for Logistic Regression**\n",
    "\n",
    "$$ Pr[Y=y^i|x^i;w] = g(y^i*w^Tx^i)$$\n",
    "\n",
    "Given a training set $S$, what is the most likely $w$, given the training set?\n",
    "\n",
    "$$ Likelihood(w) = \\prod_{i=1}^{m}p(Y=y^i|x^i;w) = \\prod_{i=1}^{m}g(y^i*w^Tx^i) $$\n",
    "\n",
    "$$ Log-Likelihood(w) = -\\sum_{i=1}^{m}log\\;g(1+exp(-y^i*w^Tx^i)) $$\n",
    "\n",
    "Notice that the likelihood we just derived was the logistic loss, $L(w)$ we looked at in the last section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b413c7-cc5e-40cc-97b0-003751298909",
   "metadata": {},
   "source": [
    "## ***1.10.3 Minimizing Logistic Loss***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286fa1ee-5598-4311-bfd5-befb55e862ad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **New Goal: Minimizing L(w)**\n",
    "\n",
    "Since the goal of finding the logistic loss function was to find a convex and differentiable function, and we achieved that, we can now use **gradient descent** on logistic loss. This is **logistic regression**.\n",
    "\n",
    "#### **Computing the gradient of L(w)**\n",
    "1. Finding $ \\phi_{logistic}'(z)$:\n",
    "$$ \\phi_{logistic}(z) = log(1+e^{-z}) $$\n",
    "$$ \\phi_{logistic}'(z) = \\frac{-e^{-z}}{1+e^{-z}} = \\frac{1}{1+e^z} = -g(-z)$$\n",
    "2. Finding the partial derivative of $ \\phi_{logistic}(y*w^Tx^)$ with respect to $w_k$:\n",
    "$$ \\frac{\\partial \\;\\phi_{logistic}(y*w^Tx)}{\\partial\\; w_k} = -g(-y*w^Tx)*y*x_k$$\n",
    "\n",
    "Thus, for the entire dataset, the gradient is:\n",
    "\n",
    "$$\\nabla L(w) = -\\frac{1}{m}\\sum_{i=1}^m(-y^iw^Tx^i)*y^i*x^i$$\n",
    "\n",
    "With this formula, we can directly apply gradient descent, $w \\gets w-\\eta\\nabla L(w)$. This precisely tells us how to find the max likelihood $w$.\n",
    "\n",
    "---\n",
    "#### **Multinomial logistic regression**\n",
    "\n",
    "What happens if we have multiple labels for y? Instead of $y \\in \\{0,1\\}$, what if $y \\in \\{1, ..., k\\}$?\n",
    "\n",
    "$$ Pr[y=1|x] \\propto e^{w^{1^T}x}$$\n",
    "$$ Pr[y=j|x] \\propto e^{w^{j^T}x}$$\n",
    "$$ Pr[y=k|x] = 1-\\sum_{i=1}^{k-1} Pr[y=i]$$\n",
    "\n",
    "#### **Cross-Entropy Loss**\n",
    "\n",
    "What is the associated loss with multinomial regression? Cross-entropy loss is generalization of logistic loss. \n",
    "\n",
    "Imagine $Y$ is a vector of length $k$ with a 1 in the $j^{th}$ position, if the correct label is $j$. (This is called **one-hot encoding** of labels). Let's say our guess for the probability $y$ has the label $i$ is $P_i$.\n",
    "\n",
    "$$ P_i = -\\sum_{i=1}^k y_i\\;log(P_i) $$\n",
    "\n",
    "#### **Softmax**\n",
    "\n",
    "Softmax turns real-values in to probabilities, like how we did when we used the sigmoid function to map $w^Tx$ to a probability.\n",
    "\n",
    "Softmax takes a vector of $k$ real-values coordinates and maps them to a vector of probabilities by:\n",
    "\n",
    "$$ (z_1, ..., z_k) \\leadsto (\\frac{e^z_1}{z}, \\frac{e^z_2}{z}, ..., \\frac{e^z_k}{z}) $$ \n",
    "\n",
    "Where $z = \\sum_{i=1}^{k}e^{z_i}$. Since we divided all the values in the vector by $z$ (normalized the vector) all the values will add to 1.\n",
    "\n",
    "Thus, performing a softmax really corresponds to taking some $k$ long vector of real values, and mapping them to probabilities between 0 and 1, that all sum to 1. It corresponds to taking some real-valued scores and transforming them into probabilities that correspond to guesses for what the class label should be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e7012d-964a-45a1-9fc9-b597e99a2d8e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1a8393-2758-4438-936f-907a728446a6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
