{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "none yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {},
   "source": [
    "## ***Introduction and Construction***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2ec31-1e8c-44c3-bb67-c6f2b821879c",
   "metadata": {},
   "source": [
    "A decision tree is a boolean function (outputs true or false). At each node in the decision tree, there is a literal. At the leaves there is a fixed value which is the output.\n",
    "\n",
    "The size of the decision tree will be the number of nodes in the tree. The depth (height) of the tree is equal to the length of the longest path from the root to a leaf.\n",
    "\n",
    "*Note that for an input going into a decision tree, the x is referred to as a \"challenge\", and the y a \"label\".*\n",
    "\n",
    "Topics:\n",
    "- Heuristics for learning decision trees\n",
    "- Theoretical properties\n",
    "\n",
    "---\n",
    "\n",
    "**Example input: X ∈ {0, 1}<sup>n</sup> (bit string of n length)**\n",
    "\n",
    "The decision tree is going to encode some function f(x) into {0, 1} as follows:\n",
    "\n",
    "- At each node, the tree decides which branch to take based on the value of the literal, until it reaches the leaf.\n",
    "\n",
    "The example decision tree's depth = 2, and size = 3.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### **The machine learning problem:**\n",
    "- Given a set of labeled examples, build a tree with low error\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**Տ** = training set, where Տ is a collection of strings and 0, 1 labels.\n",
    "\n",
    "- So c is a collection of X's and y's, where X ∈ {0, 1}<sup>n</sup>, and y ∈ {0, 1}.\n",
    "<br>\n",
    "\n",
    "**Error Rate/Training Error/Emperical Error Rate** = (number of mistakes that T makes on Տ)/ size of Տ, where T is a decision tree.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### **Natural Approach for Building Decision Trees:**\n",
    "- Given a set Տ\n",
    "\n",
    "<br>\n",
    "\n",
    "- Tree 1: Very simple, trivial tree\n",
    "    - Tree is a leaf (we dont query any literals, always output 0 or 1)\n",
    "    - How do we decide what to output?\n",
    "        - Choose 1 or 0 depending on which label is more prevalent in the dataset\n",
    " <br>\n",
    "     \n",
    "- Tree 2: More advanced tree\n",
    "    - Tree has one node, the root\n",
    "    - How do we decide which literal to put at the root?\n",
    "        - You want a literal at the root that is going to discriminate between zero and one labels\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### **So how do we decide which literal to put at the root?**\n",
    "\n",
    "Define a potential function Φ(a):\n",
    "<br>&emsp;&emsp;*[English: phi of a]*\n",
    "\n",
    "$$Φ(a) = min(a, 1-a)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "So, for the trivial decision tree:\n",
    "\n",
    "Pick a literal, *x<sub>i</sub>* , then compute Φ(Pr<sub>(x, y)~Տ</sub> (y = 0))\n",
    "<br>&emsp;&emsp;*[English: Compute phi of the probability that for an example we choose from Տ that y = 0]*\n",
    "\n",
    "- Assume: 10 positive examples\n",
    "- Assume: 5 negative examples\n",
    "- What is Φ(Pr<sub>(x, y)~Տ</sub> (y = 0))?\n",
    "    - 1/3\n",
    "- *This* probability is the error rate for the trivial decistion tree.\n",
    "\n",
    "$$ Φ(Pr_{(x, y)\\textasciitilde Տ} (y = 0)) $$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Looking at the tree with one node, pick a literal, *x<sub>1</sub>*, as the root node...\n",
    "\n",
    "What label should be put on the first leaf?\n",
    "- Condition on *x<sub>1</sub>* = 0 -> output the majority value\n",
    "\n",
    "Then, for the second leaf...\n",
    "- Condition on *x<sub>1</sub>* = 1 -> output the majority value\n",
    "\n",
    "Meaning, for each option of the value of *x<sub>1</sub>*, we output the majority label for that value of *x<sub>1</sub>*.\n",
    "\n",
    "<br> \n",
    "\n",
    "**What is the new error rate?**\n",
    "\n",
    "It is a weighted average of the error of each of the new leaves. Explicitly written out, the error rate for the decision tree with one node is:\n",
    "\n",
    "$$\n",
    "Pr_{(x, y)\\textasciitildeՏ}[x_1 = 0]*Φ(Pr_{(x, y)\\textasciitilde Տ} (y = 0) | x_1 = 0) + \n",
    "Pr_{(x, y)\\textasciitilde Տ}[x_1 = 1]*Φ(Pr_{(x, y)\\textasciitilde Տ} (y = 0) | x_1 = 1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gain(x<sub>1</sub>) = Old Rate - New Rate using x<sub>1</sub>**\n",
    "<br>&emsp;&emsp;*[English: The gain of x<sub>1</sub> is the old error rate minus the new error rate using x<sub>1</sub>]*\n",
    "\n",
    "This is the gain in training error that we attained by moving from the trivial decision tree to the decision tree where we put x<sub>1</sub> at the root. We are defining it as Gain(x<sub>1</sub>).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Now we can compute the Gain(x<sub>i</sub>) of each literal, from x<sub>1</sub> to x<sub>n</sub>, and find which literal maximizes the gain and place that literal at the root of our tree. \n",
    "\n",
    "Once we have done that, each branch will now be using a subset of the original set. In this case the left branch will use the training set Տ<sub>|x<sub>1</sub>=0</sub> *[English: Տ restricted to x<sub>1</sub>=0]*, and the right branch will be using the training set Տ<sub>|x<sub>1</sub>=1</sub> *[English: Տ restricted to x<sub>1</sub>=1]*. \n",
    "\n",
    "Meaning we have two different training sets now, one for the left subtree and one for the right subtree. We repeat the process of computing what literal should be at the root of the next subtrees and continue until the tree has been completed.\n",
    "\n",
    "Is this computationally feasible?\n",
    "\n",
    "    It depends on what the functions are. In this case, the gain function is relatively easy to compute, but also consider how large of a tree that you want to build. Also, if you start building trees that are extremely or exponentially large in terms of the features we have, that is not going to be computationally feasible. So we are going to need some sort of stopping criterion. The stopping criterion will be covered later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed5cd-43a9-4384-be36-0a59ee244e86",
   "metadata": {},
   "source": [
    "## ***Potential Functions and Random Forests***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb82a9-1780-4c14-9406-a59c82ba1d90",
   "metadata": {},
   "source": [
    "#### **Tree Structure**\n",
    "\n",
    "The structure of the tree is determined by the choice of potential function, $\\phi$. For example, we used $\\phi(a) = min(a, 1-a)$, which corresponded to training error. Another common potential function is $\\phi = 2\\cdot a \\cdot1-a$, the Gini function or Gini index. Comparing the two using graphs:\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.6.png\" alt=\"Professor Notes\" width=\"400\"/>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "We can see that $\\phi_1$ is not convex or differentiable at all points due to the discontinuity at .5. $\\phi_2$ is an upper bound on $\\phi_1$, and it is smooth! Because it is an upper bound:\n",
    "\n",
    "$$ small\\;values\\;of\\;\\phi_2 \\implies small\\;values\\;of\\;\\phi_1 $$\n",
    "\n",
    "This means that when $\\phi_2$ is getting smaller, $\\phi_1$ is getting smaller too.\n",
    "\n",
    "#### **Example with Gini Index**\n",
    "\n",
    "Let's look at an example using the following table, $S$, and potential function $\\phi(a) = 2\\cdot a \\cdot (1-a)$.\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "x_1 & x_2 & \\text{Pos} & \\text{Neg} \\\\\n",
    "\\hline\n",
    "0 & 0 & 1 & 1 \\\\\n",
    "\\hline\n",
    "0 & 1 & 2 & 1 \\\\\n",
    "\\hline\n",
    "1 & 0 & 3 & 1 \\\\\n",
    "\\hline\n",
    "1 & 1 & 4 & 2 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "***What is  $\\;\\phi_{S}(Pr(Neg))$?***\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.7.png\" alt=\"Professor Notes\" width=\"400\"/>\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "As we can see worked out, it is $\\frac{4}{9}$ for the trivial tree.\n",
    "\n",
    "Now, **should we pick $x_1$ or $x_2$ to be at the root**?\n",
    "\n",
    "Let's look at $x_1$:\n",
    "\n",
    "$$ Pr(x_1=0)\\cdot (Pr(Neg|x_1=0))+Pr(x_1=1)\\cdot (Pr(Neg|x_1=1)) $$\n",
    "$$ = \\frac{1}{3}\\cdot 2\\cdot \\frac{2}{5}\\cdot \\frac{3}{5} + \\frac{2}{3}\\cdot 2 \\cdot\\frac{3}{10} \\cdot \\frac{7}{10} $$\n",
    "$$ = \\frac{11}{25} $$\n",
    "\n",
    "$\\frac{11}{25} $ is slightly better than $\\frac{4}{9}$, so $x_1$ is a slight improvement in choice over the trivial tree.\n",
    "\n",
    "Let's look at $x_2$:\n",
    "\n",
    "$$ Pr(x_2=0)\\cdot (Pr(Neg|x_2=0))+Pr(x_2=1)\\cdot (Pr(Neg|x_2=1)) $$\n",
    "$$ = \\frac{2}{5}\\cdot 2\\cdot \\frac{1}{3}\\cdot \\frac{2}{3} + \\frac{3}{5}\\cdot 2 \\cdot\\frac{3}{9} \\cdot \\frac{7}{9} $$\n",
    "$$ = \\frac{4}{9} $$\n",
    "\n",
    "We can now calculate the gain of each $x$ being the root. For $x_1$:\n",
    "\n",
    "$$ \\frac{4}{9} - {11}{25} > 0 $$\n",
    "\n",
    "And for $x_2$:\n",
    "\n",
    "$$ \\frac{4}{9} - \\frac{4}{9} = 0 $$\n",
    "\n",
    "$x_1$ is the variable with the greatest gain, so $x_1$ is the best choice of root node.\n",
    "\n",
    "#### **When to Stop**\n",
    "\n",
    "***Question: When should we stop?***\n",
    "\n",
    "There are many answers...\n",
    "- Stop when the gain is extremely small for all literals\n",
    "- Pruning: build an enormous tree, have some parameter indicating how many nodes desired. Start at the bottom of the tree and move upward, removing branches that have little effect on the rest of the tree.\n",
    "\n",
    "#### **Random Forests**\n",
    "\n",
    "This is the practice of building many small decision trees, and taking the majority vote of the resulting trees.\n",
    "\n",
    "***Question: How do we build many decision trees?***\n",
    "\n",
    "Algorithm:\n",
    "- Take training set $S$\n",
    "- Randomly subsample from $S$ to create $S'$ (can subsample with or without replacement)\n",
    "- Randomly choose some features ${x_1,\\dots,x_n}$ of size $k$\n",
    "- Build a decision tree using $S'$ and the $k$ random features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {},
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": [
    "**[Understanding Machine Learning: From Theory to Algorithms, Chapter 18](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
