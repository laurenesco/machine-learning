{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "none yet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {},
   "source": [
    "## ***Introduction and Construction***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2ec31-1e8c-44c3-bb67-c6f2b821879c",
   "metadata": {},
   "source": [
    "A decision tree is a boolean function (outputs true or false). At each node in the decision tree, there is a literal. At the leaves there is a fixed value which is the output.\n",
    "\n",
    "The size of the decision tree will be the number of nodes in the tree. The depth (height) of the tree is equal to the length of the longest path from the root to a leaf.\n",
    "\n",
    "*Note that for an input going into a decision tree, the x is referred to as a \"challenge\", and the y a \"label\".*\n",
    "\n",
    "Topics:\n",
    "- Heuristics for learning decision trees\n",
    "- Theoretical properties\n",
    "\n",
    "---\n",
    "\n",
    "**Example input: X ∈ {0, 1}<sup>n</sup> (bit string of n length)**\n",
    "\n",
    "The decision tree is going to encode some function f(x) into {0, 1} as follows:\n",
    "\n",
    "- At each node, the tree decides which branch to take based on the value of the literal, until it reaches the leaf.\n",
    "\n",
    "The example decision tree's depth = 2, and size = 3.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### The machine learning problem:\n",
    "- Given a set of labeled examples, build a tree with low error\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**Տ** = training set, where Տ is a collection of strings and 0, 1 labels.\n",
    "\n",
    "- So c is a collection of X's and y's, where X ∈ {0, 1}<sup>n</sup>, and y ∈ {0, 1}.\n",
    "<br>\n",
    "\n",
    "**Error Rate/Training Error/Emperical Error Rate** = (number of mistakes that T makes on Տ)/ size of Տ, where T is a decision tree.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### Natural approach for building decision trees:\n",
    "- Given a set Տ\n",
    "\n",
    "<br>\n",
    "\n",
    "- Tree 1: Very simple, trivial tree\n",
    "    - Tree is a leaf (we dont query any literals, always output 0 or 1)\n",
    "    - How do we decide what to output?\n",
    "        - Choose 1 or 0 depending on which label is more prevalent in the dataset\n",
    " <br>\n",
    "     \n",
    "- Tree 2: More advanced tree\n",
    "    - Tree has one node, the root\n",
    "    - How do we decide which literal to put at the root?\n",
    "        - You want a literal at the root that is going to discriminate between zero and one labels\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### So how do we decide which literal to put at the root?\n",
    "\n",
    "Define a potential function Φ(a):\n",
    "<br>&emsp;&emsp;*[English: phi of a]*\n",
    "\n",
    "$$Φ(a) = min(a, 1-a)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "So, for the trivial decision tree:\n",
    "\n",
    "Pick a literal, *x<sub>i</sub>* , then compute Φ(Pr<sub>(x, y)~Տ</sub> (y = 0))\n",
    "<br>&emsp;&emsp;*[English: Compute phi of the probability that for an example we choose from Տ that y = 0]*\n",
    "\n",
    "- Assume: 10 positive examples\n",
    "- Assume: 5 negative examples\n",
    "- What is Φ(Pr<sub>(x, y)~Տ</sub> (y = 0))?\n",
    "    - 1/3\n",
    "- *This* probability is the error rate for the trivial decistion tree.\n",
    "\n",
    "$$ Φ(Pr_{(x, y)\\textasciitilde Տ} (y = 0)) $$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Looking at the tree with one node, pick a literal, *x<sub>1</sub>*, as the root node...\n",
    "\n",
    "What label should be put on the first leaf?\n",
    "- Condition on *x<sub>1</sub>* = 0 -> output the majority value\n",
    "\n",
    "Then, for the second leaf...\n",
    "- Condition on *x<sub>1</sub>* = 1 -> output the majority value\n",
    "\n",
    "Meaning, for each option of the value of *x<sub>1</sub>*, we output the majority label for that value of *x<sub>1</sub>*.\n",
    "\n",
    "<br> \n",
    "\n",
    "**What is the new error rate?**\n",
    "\n",
    "It is a weighted average of the error of each of the new leaves. Explicitly written out, the error rate for the decision tree with one node is:\n",
    "\n",
    "$$\n",
    "Pr_{(x, y)\\textasciitildeՏ}[x_1 = 0]*Φ(Pr_{(x, y)\\textasciitilde Տ} (y = 0) | x_1 = 0) + \n",
    "Pr_{(x, y)\\textasciitilde Տ}[x_1 = 1]*Φ(Pr_{(x, y)\\textasciitilde Տ} (y = 0) | x_1 = 1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gain(x<sub>1</sub>) = Old Rate - New Rate using x<sub>1</sub>**\n",
    "<br>&emsp;&emsp;*[English: The gain of x<sub>1</sub> is the old error rate minus the new error rate using x<sub>1</sub>]*\n",
    "\n",
    "This is the gain in training error that we attained by moving from the trivial decision tree to the decision tree where we put x<sub>1</sub> at the root. We are defining it as Gain(x<sub>1</sub>).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.2.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Now we can compute the Gain(x<sub>i</sub>) of each literal, from x<sub>1</sub> to x<sub>n</sub>, and find which literal maximizes the gain and place that literal at the root of our tree. \n",
    "\n",
    "Once we have done that, each branch will now be using a subset of the original set. In this case the left branch will use the training set Տ<sub>|x<sub>1</sub>=0</sub> *[English: Տ restricted to x<sub>1</sub>=0]*, and the right branch will be using the training set Տ<sub>|x<sub>1</sub>=1</sub> *[English: Տ restricted to x<sub>1</sub>=1]*. \n",
    "\n",
    "Meaning we have two different training sets now, one for the left subtree and one for the right subtree. We repeat the process of computing what literal should be at the root of the next subtrees and continue until the tree has been completed.\n",
    "\n",
    "Is this computationally feasible?\n",
    "\n",
    "    It depends on what the functions are. In this case, the gain function is relatively easy to compute, but also consider how large of a tree that you want to build. Also, if you start building trees that are extremely or exponentially large in terms of the features we have, that is not going to be computationally feasible. So we are going to need some sort of stopping criterion. The stopping criterion will be covered later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed5cd-43a9-4384-be36-0a59ee244e86",
   "metadata": {},
   "source": [
    "## ***Potential Functionas and Random Forests***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb82a9-1780-4c14-9406-a59c82ba1d90",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {},
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": [
    "**[Understanding Machine Learning: From Theory to Algorithms, Chapter 18](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)** "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
