{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# Linear Regression #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": [
    "second derivate test \\\n",
    "variance\\\n",
    "covariance\\\n",
    "span\\\n",
    "projection\\\n",
    "transpose\\\n",
    "normal equations\\\n",
    "pseudo-inverse\\\n",
    "likelihood\\\n",
    "derivative rules\\\n",
    "pdf of the gaussian\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {},
   "source": [
    "## ***Introduction***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2ec31-1e8c-44c3-bb67-c6f2b821879c",
   "metadata": {},
   "source": [
    "#### **Background**\n",
    "\n",
    "Linear regression is a core task in multiple fields including statistics, computer science, and machine learning. It is the problem of fitting a line to data.\n",
    "\n",
    "One difference from classification to regression, is our labels in regression will be real-valued, not just 1 or 0.\n",
    "\n",
    "#### **Deriving the Regression Function**\n",
    "\n",
    "Let's say we have two random variables $X$, $Y$. We want to predict $Y$, the label.\n",
    "\n",
    "In one scenario, assume we are not able to see $X$. Given that $(X,Y) \\textasciitilde \\mathbf{D}$, our optimal guess for $Y$ is simply $\\mathbf{E}[Y]$. We would then measure our loss using square loss: $(Prediction-Y)^2$\n",
    "\n",
    "In the scenario where we do get to see $X$, then the optimal prediction for $Y$ will be $\\mathbf{E}[Y|X]$, the expected value of $Y$ conditioned on $X$.\n",
    "\n",
    "This value, $\\mathbf{E}[Y|X]$, is a function of the random variable $X$, written $f(X)$. We call this function the **regression** function.\n",
    "\n",
    "A major obstacle is that $f(x)$ might be unknown, or very hard to compute.\n",
    "\n",
    "#### **Introducing the Coefficients**\n",
    "\n",
    "Linear regression asks the following: \n",
    "\n",
    "**Given $X$, what linear function of $X$ should we use to predict $Y$?**\n",
    "\n",
    "Essentially, we want to know which line best fits the data with respect to square loss. We want to learn coefficients $\\beta_0$ and $\\beta_1$ to minimize $\\mathbf{E}_{(X,Y) \\textasciitilde \\mathbf{D}}([(Y-(\\beta_0 + \\beta_1x))^2]$, the expected value of the square loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed5cd-43a9-4384-be36-0a59ee244e86",
   "metadata": {},
   "source": [
    "## ***Finding the Betas***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb82a9-1780-4c14-9406-a59c82ba1d90",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### **Our Minimization Function**\n",
    "\n",
    "We are going to draw a training set of size $m$: $(x^1, y^1),\\; \\dots \\;, (x^m,y^m)$, where $x$ and $y$ are scalars, so this is simple linear regression. In this case, we want to:\n",
    "\n",
    "$$ \\underset{\\beta_0, \\beta_1}{\\min} \\frac{1}{m}\\sum_{j=1}^m(y^j-(\\beta_o+\\beta_1 x^j))^2$$\n",
    "\n",
    "#### **How to Find Beta 0 and Beta 1**\n",
    "\n",
    "We will take the derivate with respect to $\\beta_0$ and $\\beta_1$, and set them equal to 0. Let's fix $\\mathcal{l}$ as $\\sum_{j=1}^m(y^j-(\\beta_o+\\beta_1 x^j))^2$ from the minimization function. Now we can compute the partial derivative of $\\mathcal{l}$ with respect to $\\beta_0$.\n",
    "\n",
    "Note: We know from calculus that setting a derivative equal to zero yields a critical point, but how can we be sure that this is the global minimum? Since this function is convex we can assume so. However we can also take the second derivative and apply the second derivative test. You can also get the Hessian from the second derivate, a semi-definite matrix, and look at the eigenvalues, which we will do later in the course.\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{l}}{\\partial \\beta_0} = \\frac{1}{m}\\sum_{j=1}^m(y^j-\\beta_0-\\beta_1x^j)(-2)$$\n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{l}}{\\partial \\beta_1} = \\frac{1}{m}\\sum_{j=1}^m(y^j-\\beta_0-\\beta_1x^j)(-2x^j)$$\n",
    "\n",
    "Removing the $-2$ multiplier (because it can easily be divided out), we have:\n",
    "\n",
    "$$ \\frac{1}{m}\\sum_{j=1}^m(y^j-\\beta_0-\\beta_1x^j) = 0 $$\n",
    "\n",
    "$$\\frac{1}{m}\\sum_{j=1}^m(y^j-\\beta_0-\\beta_1x^j)(x^j) = 0$$\n",
    "\n",
    "Solving for the betas:\n",
    "\n",
    "$$ \\beta_0 = \\bar{y}-\\beta_1\\bar{x} $$\n",
    "\n",
    "$$ \\beta_1 = \\frac{\\bar{xy}-\\bar{x}\\cdot\\bar{y}}{\\bar{x^2}-(\\bar{x})^2} $$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.7.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "#### **Beta 1 and the Variance of X**\n",
    "\n",
    "Notice that the denominator in our expression for $\\beta_1$ looks a lot like the expression for the variance of $x$:\n",
    "\n",
    "$$ \\bar{x^2}-(\\bar{x})^2 $$\n",
    "$$ Var(X) = \\mathbf{E}[X^2]âˆ’(\\mathbf{E}[X])^2 $$\n",
    "\n",
    "And the numerator looks a lot like the expression for the covariance of $x$ and $y$:\n",
    "\n",
    "$$ \\bar{xy}-\\bar{x}\\cdot\\bar{y} $$\n",
    "$$ Cov(X,Y) = \\mathbf{E}[X\\cdot Y] - \\mathbf{E}[X] \\cdot \\mathbf{E}[Y] $$\n",
    "\n",
    "So it seems $\\beta_1$ is the covariance of the slope of the line ($x, y$), divided by the variance of $x$.\n",
    "\n",
    "$$ \\beta_1 = \\frac{Cov(X,Y)}{Var(X)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30eefce1-6926-4e40-bb51-c850c23f3208",
   "metadata": {},
   "source": [
    "## ***Regression with Multiple Variables***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47367318-9c44-41f9-acd7-2a9cdf4fb012",
   "metadata": {},
   "source": [
    "#### **Defining the Problem**\n",
    "Now, instead of assuming $X$ is a scalar, $X$ is an $n$ dimensional vector, but $Y$ is still a scalar:\n",
    "\n",
    "$$ X \\in \\mathbf{R}^n\\;\\;\\;\\; y \\in \\mathbf{R} $$\n",
    "\n",
    "So we are fitting a line to $n$-dimensional data.\n",
    "\n",
    "Consider a matrix $X$, that is an $m \\times n$ matrix. \n",
    "- $X$ has $m$ rows, where each row is equal to $X^i$ drawn from $\\mathcal{D}$. Each row is an $n$ dimensional data point.\n",
    "- $X$ has $n$ columns becuase each point is in $\\mathbf{R}^n$. $X$ has $n$ features.\n",
    "\n",
    "We will still have a vector $y \\in \\mathbf{R}^m$ corresponding to the labels for these $m$ points.\n",
    "\n",
    "**The goal** is to find a vector $w \\in \\mathbf{R}^n$ that minimizes $||X\\cdot w-y||^2_2$\n",
    "\n",
    "Example, given: \n",
    "\n",
    "$$ x^1 = x^1_1, \\dots , x^1_n \\;\\;\\;\\; y^1 $$\n",
    "\n",
    "Then: \n",
    "\n",
    "$$ (y-(x^1_1 w_1 + \\dots +x^1_n w_n))^2 $$\n",
    "\n",
    "#### **Formal Problem Statement**\n",
    "\n",
    "$$ \\underset{w}{\\min} ||Xw-y||^2_2 $$\n",
    "\n",
    "#### **How to Find w**\n",
    "\n",
    "Let's find $w$ by using geometric concepts. $Xw$ is a vector in the span of the columns of $X$. The point $y \\in \\mathbf{r}^n$ is not necessarily in the span of $X$.\n",
    "\n",
    "What point should we pick in the span of $X$ to best approximate $y$, geometrically speaking? We should take the orthogonal projection of $y$ down to the span of $X$, and that is the optimal point. We will call this point $Xw$, and that is the point we should choose.\n",
    "\n",
    "The vector $y-Xw$ line from the point $y$ to the point $Xw$, which is orthogonal to  $X$.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.7.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "Now we can take the vector $y-Xw$ and, since it is orthogonal to $X$, do the following:\n",
    "\n",
    "$$ X^T\\cdot (y-Xw) = 0 $$\n",
    "$$ X^Ty-X^TXw = 0 $$\n",
    "$$ X^Ty = X^TXw $$\n",
    "$$ (X^TX)^{-1}X^Ty = w $$\n",
    "\n",
    "Thus, we have solved for $w$.\n",
    "\n",
    "#### **The Normal Equations** \n",
    "\n",
    "$ (X^TX)^{-1}X^Ty = w $ is called the normal equation. And there are a few issues with the normal equations.\n",
    "\n",
    "1. What if $X^TX$ is not invertible?\n",
    "- We end up using something called a pseudo-inverse instead of a true inverse. This means that there will be multiple choices of $w$ that will give you this minimum square loss.\n",
    "\n",
    "2. What is the running time for computing $w$, in terms of $m$ and $n$?\n",
    "- There are some matrix inverses going on here. Roughly $\\mathcal{O}(n^3 + m\\cdot n^2) $. We will soon see faster algorithms, specifically using gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b538c2-528f-40e5-8724-e72f30fd0036",
   "metadata": {},
   "source": [
    "## ***Maximum Likelihood***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b3cf0c-981e-45cf-aa6b-ffa3d7783401",
   "metadata": {},
   "source": [
    "#### **Assumption**\n",
    "\n",
    "We will be looking at the \"simple\" linear regression case, with scalar $X$, and assume $y = \\beta_0 + \\beta_1x + \\epsilon$, where $\\epsilon$ is a random noise variable where $\\epsilon \\in N(0, \\sigma^2)$ gaussian distribution.\n",
    "\n",
    "Imagine we have drawn $x^1, \\dots, x^m$ and $y^1, \\dots, y^m$. We want to understand: for a fixed choice of $\\beta_0$ and $\\beta_1$ ($\\sigma^2$ is known), **what is the probability that we see $(x^1, y^1)\\dots (x^m,y^m)$**.\n",
    "\n",
    "#### **Likelihood Function**\n",
    "\n",
    "The likelihood function is the probability of seeing the training set given a choice $\\beta_0$ and $\\beta_1$ of our parameters:\n",
    "\n",
    "$$ \\prod_{i=1}^m P(y^i|X^i;\\beta_0, \\beta_1) $$\n",
    "\n",
    "With the probability of choosing $y$, or $P(y^i|X^i;\\beta_0, \\beta_1)$ plugged in (something about the PDF of the gaussian and $\\epsilon$):\n",
    "\n",
    "$$ = \\prod_{i=1}^m \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\cdot \\mathcal{e}^{\\frac{-(y^i-(\\beta_0+\\beta_1x^i))^2}{2\\sigma^2}} $$\n",
    "\n",
    "This is the likelihood of our training set. Let's call this quantity $L(\\beta_0,\\beta_1)$. We want to choose $\\beta_0$ and $\\beta_1$ that maximizes this likelihood.\n",
    "\n",
    "#### **Maximizing Likelihood**\n",
    "\n",
    "Instead of directly maximizing likelihood, we will maximize log-likelihood $Log(L(\\beta_0,\\beta_1))$. Because we have a product, it is often convenient to take the log.\n",
    "\n",
    "$$Log(L(\\beta_0,\\beta_1)) = log\\prod_{i=1}^m P(y^i|X^i;\\beta_0,\\beta_1) $$\n",
    "$$ = \\sum_{i=1}^m log (P(y^i|X^i;\\beta_0,\\beta_1)) $$\n",
    "$$ = \\frac{-m}{2}log\\;2\\;\\pi-m\\;log\\;\\sigma-\\frac{1}{2\\sigma^2}\\sum_{i=1}^m (y^i-(\\beta_0+\\beta_1x^i))^2 $$\n",
    "\n",
    "Notice that the only term that will be affected by our choice of $\\beta$'s is the last term. Also notice that the last term is the **least-squares estimate** for simple linear regression we looked at earlier.\n",
    "\n",
    "#### **Interpreting Coefficients**\n",
    "\n",
    "There are two interpretations for coefficients in lienar regression:\n",
    "- Geometric; coefficients of the line that minimizes squared distance from the line to our labels\n",
    "- Statistical; coefficients that give you the maximum likelihood estimator for a training set generated per the assumtion, in this case $y \\textasciitilde N(\\beta_0+\\beta_1x^i, \\epsilon))^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {},
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
