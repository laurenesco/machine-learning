{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3514d59-d7c3-4f7d-bd88-c37fefed19c3",
   "metadata": {},
   "source": [
    "# 1.1 Mistake-bounded Model of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5371b-4955-4d41-9988-5b469e28db9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2470dbf-94ab-4738-8e2d-36ba84808648",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Analogy for mistake-bounded model: \n",
    "\n",
    "    Image an email spam filtering program that had the 100% guarantee that it would only mislabel 100 emails. No matter if you have 30 emails or 30,000 emails in your inbox, the program will only make 100 mistakes.\n",
    "\n",
    "---\n",
    "\n",
    "In this model we have a <b>\"Learner\"</b>, which takes in data points. Once it receives a data point, it responds with its guess for the classification of that data point. There is also the <b>\"Teacher\"</b>, which responds to the classification guess with whether the guess was correct or incorrect. When the Teacher tells the Learner that it made a mistake, a counter for the number of mistakes increases by one. However, also when the Learner makes a mistake, it learns from the mistake, updating its internal state.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.1.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "We say a Learner has mistake-bound <i>t</i> if for every sequence of challenges, Learner makes at most <i>t</i> mistakes.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**ùíû (Script C) definition:**\n",
    "<br>ùíû = {monotone disjunctions on n variables} \n",
    "<br>&emsp;&emsp;*English: Script C is equal to the set of all monotone disjunctions on n variables*\n",
    "<br>&emsp;&emsp;*(Note: Called monotone because there are no negations.)*\n",
    "<br>Domain = {0,1}<sub>n</sub> \n",
    "<br>&emsp;&emsp;*English: Domain is equal to the set of 0, 1 to the n (i.e., bit strings of length n)*\n",
    "\n",
    "Some functions in ùíû:\n",
    "- x1 ‚à® x3 - *Evaluates to 1 when given a bit string that has a one in the first or third position.*\n",
    "- f(x) = x1 ‚à® x7 ‚à® x9 - *Evaluates to 1 when given a bit string that has a one in the first, seventh, or ninth position.*\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.1.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "*f* ‚àà ùíû, so *f* is a monotone disjunction. The Learner does not know that *f* is a monotone disjunction. The Learner is fed a string in the domain, and responds with a 0 or a 1. The Teacher then responds with \"correct\" if the guess equals f(x), or \"mistake\" otherwise.\n",
    "\n",
    "If the Learner is giving a guess, 0 or 1, and the guess equals f(x), then nothing happens and the Learner moves on to the next input. If the Teacher replies that the guess was a mistake, then the Learner will update its state and recieve another input. \n",
    "\n",
    "In either case, the Learner is learning something. If the Learner was correct, it learned that it knew f(x). If it was incorrect, it still knows f(x) because f(x) is simply the opposite of the response the learner had given.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.1.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question:\n",
    "Can you come up with a Learner/algorithm with mistake bound at most *n*?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455aef9b-c425-49b3-94b6-c4ca295a6153",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95a5ac-36e9-4d88-a609-5e36dd9e56fa",
   "metadata": {},
   "source": [
    "**[On-line Algorithms in Machine Learning](C:\\Users\\laesc\\OneDrive\\Documents\\college\\ut%20austin\\machine%20learning\\4%20-%20resources\\online-algorithms-in-machine-learning.pdf)** (Local link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35064c2a-76f7-4496-8e7c-1c02b9fcf8e1",
   "metadata": {},
   "source": [
    "# 1.2 Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d34189-8b10-432a-9c47-b696e1134afa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86424282-b5cc-457e-998b-b43f9e358b47",
   "metadata": {},
   "source": [
    "Decision trees are a powerful way to classify data with a lot of nice theoretical properties. \n",
    "\n",
    "A decision tree is a boolean function (outputs true or false). At each node in the decision tree, there is a literal. At the leaves there is a fixed value which is the output.\n",
    "\n",
    "The size of the decision tree will be the number of nodes in the tree \n",
    "The depth (height) of the tree is equal to the length of the longest path from the root to a leaf.\n",
    "\n",
    "Topics:\n",
    "- Heuristics for learning decision trees\n",
    "- Theoretical properties\n",
    "\n",
    "---\n",
    "\n",
    "**Example input: X ‚àà {0, 1}<sup>n</sup> (bit string of n length)**\n",
    "\n",
    "The decision tree is going to encode some function f(x) into {0, 1} as follows:\n",
    "\n",
    "- At each node, the tree decides which branch to take based on the value of the literal, until it reaches the leaf.\n",
    "\n",
    "The example decision tree's depth = 2, and size = 3.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### The machine learning problem:\n",
    "- Given a set of labeled examples, build a tree with low error\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**’è** = training set, where ’è is a collection of strings and 0, 1 labels.\n",
    "\n",
    "- So ’è is a collection of X's and y's, where X ‚àà {0, 1}<sup>n</sup>, and y ‚àà {0, 1}.\n",
    "<br>\n",
    "\n",
    "**Error Rate/Training Error/Emperical Error Rate** = (number of mistakes that T makes on ’è)/ size of ’è, where T is a decision tree.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### Natural approach for building decision trees:\n",
    "- Given a set ’è\n",
    "\n",
    "<br>\n",
    "\n",
    "- Tree 1: Very simple, trivial tree\n",
    "    - Tree is a leaf (we dont query any literals, always output 0 or 1)\n",
    "    - How do we decide what to output?\n",
    "        - Choose 1 or 0 depending on which label is more prevalent in the dataset\n",
    " <br>\n",
    "     \n",
    "- Tree 2: More advanced tree\n",
    "    - Tree has one node, the root\n",
    "    - How do we decide which literal to put at the root?\n",
    "        - You want a literal at the root that is going to discriminate between zero and one labels\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### So how do we decide which literal to put at the root?\n",
    "\n",
    "Define a potential function Œ¶(a):\n",
    "<br>&emsp;&emsp;*[English: phi of a]*\n",
    "\n",
    "$$Œ¶(a) = min(a, 1-a)$$\n",
    "\n",
    "\n",
    "Pick a literal, *x<sub>i</sub>* , then compute Œ¶(Pr<sub>(x, y)~’è</sub> (y = 0))\n",
    "<br>&emsp;&emsp;*[English: Compute phi of the probability that an example we choose from ’è that y = 0]*\n",
    "\n",
    "- Assume: 10 positive examples\n",
    "- Assume: 5 negative examples\n",
    "- What is Œ¶(Pr<sub>(x, y)~’è</sub> (y = 0))?\n",
    "    - 1/3\n",
    "- *This* probability is the error rate for the trivial decistion tree.\n",
    "\n",
    "$$ Œ¶(Pr_{(x, y)~’è} (y = 0)) $$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "But, we would like to do better than that. Looking at the tree with one node, pick a literal, *x<sub>1</sub>*, as the root node...\n",
    "\n",
    "What label should be put on the first leaf?\n",
    "- Condition on *x<sub>1</sub>* = 0 -> output the majority value\n",
    "\n",
    "Then, for the second leaf...\n",
    "- Condition on *x<sub>1</sub>* = 1 -> output the majority value\n",
    "\n",
    "Meaning, for each option of the value of *x<sub>1</sub>*, we output the majority label for that value of *x<sub>1</sub>*.\n",
    "\n",
    "<br> \n",
    "\n",
    "**What is the new error rate?**\n",
    "\n",
    "It is a weighted average of the error of each of the new leaves. Explicitly written out, the error rate for the decision tree with one node is:\n",
    "\n",
    "$$\n",
    "Pr_{(x, y)~’è}[x_1 = 0]*Œ¶(Pr_{(x, y)~’è} (y = 0) | x_1 = 0) + \n",
    "Pr_{(x, y)~’è}[x_1 = 1]*Œ¶(Pr_{(x, y)~’è} (y = 0) | x_1 = 1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gain(x<sub>1</sub>) = Old Rate - New Rate using x<sub>1</sub>**\n",
    "<br>&emsp;&emsp;*[English: The gain of x<sub>1</sub> is the old error rate minus the new error rate using x<sub>1</sub>]*\n",
    "\n",
    "This is the gain in training error that we attained by moving from the trivial decision tree to the decision tree where we put x<sub>1</sub> at the root. We are defining it as Gain(x<sub>1</sub>).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Now we can compute the Gain(x<sub>i</sub>) of each literal, from x<sub>1</sub> to x<sub>n</sub>, and find which literal maximizes the gain and place that literal at the root of our tree. \n",
    "\n",
    "Once we have done that, each branch will now be using a subset of the original set. In this case the left branch will use the training set ’è<sub>|x<sub>1</sub>=0</sub> *[English: ’è restricted to x<sub>1</sub>=0]*, and the right branch will be using the training set ’è<sub>|x<sub>1</sub>=1</sub> *[English: ’è restricted to x<sub>1</sub>=1]*. \n",
    "\n",
    "Meaning we have two different training sets now, one for the left subtree and one for the right subtree. We repeat the process of computing what literal should be at the root of the next subtrees and continue until the tree has been completed.\n",
    "\n",
    "Is this computationally feasible?\n",
    "\n",
    "    It depends on what the functions are. In this case, the gain function is relatively easy to compute, but also consider how large of a tree that you want to build. Also, if you start building trees that are extremely or exponentially large in terms of the features we have, that is not going to be computationally feasible. So we are going to need some sort of stopping criterion. The stopping criterion will be covered later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd79ca8b-7cc8-4232-b233-9c11ef880aff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5919c6-0c0c-49c2-891e-b5d05b61e643",
   "metadata": {},
   "source": [
    "**[Understanding Machine Learning: From Theory to Algorithms, Chapter 18](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)** (Internet link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
