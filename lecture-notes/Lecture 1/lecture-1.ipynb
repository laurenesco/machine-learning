{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3514d59-d7c3-4f7d-bd88-c37fefed19c3",
   "metadata": {},
   "source": [
    "# **1.1 Mistake-bounded Model of Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed5371b-4955-4d41-9988-5b469e28db9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## &emsp;&emsp;Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2470dbf-94ab-4738-8e2d-36ba84808648",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Analogy for mistake-bounded model: \n",
    "\n",
    "    Image an email spam filtering program that had the 100% guarantee that it would only mislabel 100 emails. No matter if you have 30 emails or 30,000 emails in your inbox, the program will only make 100 mistakes.\n",
    "\n",
    "---\n",
    "\n",
    "In this model we have a <b>\"Learner\"</b>, which takes in data points. Once it receives a data point, it responds with its guess for the classification of that data point. There is also the <b>\"Teacher\"</b>, which responds to the classification guess with whether the guess was correct or incorrect. When the Teacher tells the Learner that it made a mistake, a counter for the number of mistakes increases by one. However, also when the Learner makes a mistake, it learns from the mistake, updating its internal state.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.1.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "We say a Learner has mistake-bound <i>t</i> if for every sequence of challenges, Learner makes at most <i>t</i> mistakes.\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**ùíû (Script C) definition:**\n",
    "<br>ùíû = {monotone disjunctions on n variables} \n",
    "<br>&emsp;&emsp;*English: Script C is equal to the set of all monotone disjunctions on n variables*\n",
    "<br>&emsp;&emsp;*(Note: Called monotone because there are no negations.)*\n",
    "<br>Domain = {0,1}<sub>n</sub> \n",
    "<br>&emsp;&emsp;*English: Domain is equal to the set of 0, 1 to the n (i.e., bit strings of length n)*\n",
    "\n",
    "Some functions in ùíû:\n",
    "- x1 ‚à® x3 - *Evaluates to 1 when given a bit string that has a one in the first or third position.*\n",
    "- f(x) = x1 ‚à® x7 ‚à® x9 - *Evaluates to 1 when given a bit string that has a one in the first, seventh, or ninth position.*\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.1.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "*f* ‚àà ùíû, so *f* is a monotone disjunction. The Learner does not know that *f* is a monotone disjunction. The Learner is fed a string in the domain, and responds with a 0 or a 1. The Teacher then responds with \"correct\" if the guess equals f(x), or \"mistake\" otherwise.\n",
    "\n",
    "If the Learner is giving a guess, 0 or 1, and the guess equals f(x), then nothing happens and the Learner moves on to the next input. If the Teacher replies that the guess was a mistake, then the Learner will update its state and recieve another input. \n",
    "\n",
    "In either case, the Learner is learning something. If the Learner was correct, it learned that it knew f(x). If it was incorrect, it still knows f(x) because f(x) is simply the opposite of the response the learner had given.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.1.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "### Question:\n",
    "Can you come up with a Learner/algorithm with mistake bound at most *n*?\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455aef9b-c425-49b3-94b6-c4ca295a6153",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## &emsp;&emsp;Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d95a5ac-36e9-4d88-a609-5e36dd9e56fa",
   "metadata": {},
   "source": [
    "**[On-line Algorithms in Machine Learning](C:\\Users\\laesc\\OneDrive\\Documents\\college\\ut%20austin\\machine%20learning\\4%20-%20resources\\online-algorithms-in-machine-learning.pdf)** (Local link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35064c2a-76f7-4496-8e7c-1c02b9fcf8e1",
   "metadata": {},
   "source": [
    "# **1.2 Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d34189-8b10-432a-9c47-b696e1134afa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## &emsp;&emsp;Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86424282-b5cc-457e-998b-b43f9e358b47",
   "metadata": {},
   "source": [
    "A decision tree is a boolean function (outputs true or false). At each node in the decision tree, there is a literal. At the leaves there is a fixed value which is the output.\n",
    "\n",
    "The size of the decision tree will be the number of nodes in the tree. The depth (height) of the tree is equal to the length of the longest path from the root to a leaf.\n",
    "\n",
    "*Note that for an input going into a decision tree, the x is referred to as a \"challenge\", and the y a \"label\".*\n",
    "\n",
    "Topics:\n",
    "- Heuristics for learning decision trees\n",
    "- Theoretical properties\n",
    "\n",
    "---\n",
    "\n",
    "**Example input: X ‚àà {0, 1}<sup>n</sup> (bit string of n length)**\n",
    "\n",
    "The decision tree is going to encode some function f(x) into {0, 1} as follows:\n",
    "\n",
    "- At each node, the tree decides which branch to take based on the value of the literal, until it reaches the leaf.\n",
    "\n",
    "The example decision tree's depth = 2, and size = 3.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### The machine learning problem:\n",
    "- Given a set of labeled examples, build a tree with low error\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "**’è** = training set, where ’è is a collection of strings and 0, 1 labels.\n",
    "\n",
    "- So ’è is a collection of X's and y's, where X ‚àà {0, 1}<sup>n</sup>, and y ‚àà {0, 1}.\n",
    "<br>\n",
    "\n",
    "**Error Rate/Training Error/Emperical Error Rate** = (number of mistakes that T makes on ’è)/ size of ’è, where T is a decision tree.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### Natural approach for building decision trees:\n",
    "- Given a set ’è\n",
    "\n",
    "<br>\n",
    "\n",
    "- Tree 1: Very simple, trivial tree\n",
    "    - Tree is a leaf (we dont query any literals, always output 0 or 1)\n",
    "    - How do we decide what to output?\n",
    "        - Choose 1 or 0 depending on which label is more prevalent in the dataset\n",
    " <br>\n",
    "     \n",
    "- Tree 2: More advanced tree\n",
    "    - Tree has one node, the root\n",
    "    - How do we decide which literal to put at the root?\n",
    "        - You want a literal at the root that is going to discriminate between zero and one labels\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### So how do we decide which literal to put at the root?\n",
    "\n",
    "Define a potential function Œ¶(a):\n",
    "<br>&emsp;&emsp;*[English: phi of a]*\n",
    "\n",
    "$$Œ¶(a) = min(a, 1-a)$$\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "So, for the trivial decision tree:\n",
    "\n",
    "Pick a literal, *x<sub>i</sub>* , then compute Œ¶(Pr<sub>(x, y)~’è</sub> (y = 0))\n",
    "<br>&emsp;&emsp;*[English: Compute phi of the probability that for an example we choose from ’è that y = 0]*\n",
    "\n",
    "- Assume: 10 positive examples\n",
    "- Assume: 5 negative examples\n",
    "- What is Œ¶(Pr<sub>(x, y)~’è</sub> (y = 0))?\n",
    "    - 1/3\n",
    "- *This* probability is the error rate for the trivial decistion tree.\n",
    "\n",
    "$$ Œ¶(Pr_{(x, y)\\textasciitilde ’è} (y = 0)) $$\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Looking at the tree with one node, pick a literal, *x<sub>1</sub>*, as the root node...\n",
    "\n",
    "What label should be put on the first leaf?\n",
    "- Condition on *x<sub>1</sub>* = 0 -> output the majority value\n",
    "\n",
    "Then, for the second leaf...\n",
    "- Condition on *x<sub>1</sub>* = 1 -> output the majority value\n",
    "\n",
    "Meaning, for each option of the value of *x<sub>1</sub>*, we output the majority label for that value of *x<sub>1</sub>*.\n",
    "\n",
    "<br> \n",
    "\n",
    "**What is the new error rate?**\n",
    "\n",
    "It is a weighted average of the error of each of the new leaves. Explicitly written out, the error rate for the decision tree with one node is:\n",
    "\n",
    "$$\n",
    "Pr_{(x, y)\\textasciitilde’è}[x_1 = 0]*Œ¶(Pr_{(x, y)\\textasciitilde ’è} (y = 0) | x_1 = 0) + \n",
    "Pr_{(x, y)\\textasciitilde ’è}[x_1 = 1]*Œ¶(Pr_{(x, y)\\textasciitilde ’è} (y = 0) | x_1 = 1)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "**Gain(x<sub>1</sub>) = Old Rate - New Rate using x<sub>1</sub>**\n",
    "<br>&emsp;&emsp;*[English: The gain of x<sub>1</sub> is the old error rate minus the new error rate using x<sub>1</sub>]*\n",
    "\n",
    "This is the gain in training error that we attained by moving from the trivial decision tree to the decision tree where we put x<sub>1</sub> at the root. We are defining it as Gain(x<sub>1</sub>).\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.2.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Now we can compute the Gain(x<sub>i</sub>) of each literal, from x<sub>1</sub> to x<sub>n</sub>, and find which literal maximizes the gain and place that literal at the root of our tree. \n",
    "\n",
    "Once we have done that, each branch will now be using a subset of the original set. In this case the left branch will use the training set ’è<sub>|x<sub>1</sub>=0</sub> *[English: ’è restricted to x<sub>1</sub>=0]*, and the right branch will be using the training set ’è<sub>|x<sub>1</sub>=1</sub> *[English: ’è restricted to x<sub>1</sub>=1]*. \n",
    "\n",
    "Meaning we have two different training sets now, one for the left subtree and one for the right subtree. We repeat the process of computing what literal should be at the root of the next subtrees and continue until the tree has been completed.\n",
    "\n",
    "Is this computationally feasible?\n",
    "\n",
    "    It depends on what the functions are. In this case, the gain function is relatively easy to compute, but also consider how large of a tree that you want to build. Also, if you start building trees that are extremely or exponentially large in terms of the features we have, that is not going to be computationally feasible. So we are going to need some sort of stopping criterion. The stopping criterion will be covered later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd79ca8b-7cc8-4232-b233-9c11ef880aff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## &emsp;&emsp;Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5919c6-0c0c-49c2-891e-b5d05b61e643",
   "metadata": {},
   "source": [
    "**[Understanding Machine Learning: From Theory to Algorithms, Chapter 18](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/index.html)** (Internet link)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7fc20d8-6acc-44e4-a745-d8e8506ece47",
   "metadata": {},
   "source": [
    "# **1.3 Generalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427c260d-daa1-4684-9a56-f59a6ca1e6f5",
   "metadata": {},
   "source": [
    "## &emsp;&emsp;Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a340480-7fb0-4950-ab70-4ce8ab625edf",
   "metadata": {},
   "source": [
    "### 1.3.0 Introduction to Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab1bd7-f737-4f80-9f71-c7c497e372fe",
   "metadata": {},
   "source": [
    "#### **Generalization, or predictive power of a classifier.**\n",
    "\n",
    "What we want to understand is how well the classifier we built is going to perform when it is given data that it has not seen before. We would like to estimate this generalization error and understand when it is going to have good generalization error. This is going to lead us to the PAC model of learning, which is a foundational model of learning.\n",
    "\n",
    "- What is the \"true error\" or generalization error of a classifier?\n",
    "- Decision trees:\n",
    "    - Let us fix some tree T, created based on the rules we created above.\n",
    "    - And let us say we have a probability distribution D on new examples. \n",
    "    - So for a new challenge and a new label, and we want to know what is:\n",
    "\n",
    "$$Pr_{(x, y)\\textasciitilde D}[T(x) \\neq y]$$\n",
    "\n",
    "<br>*[English: The probability that when we randomly draw a challenge from some unknown distribution D, T(x) does not equal y]*\n",
    "\n",
    "We call this the **true error**, or generalization error, of T. Of course, we are hoping that this quantity is small.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.3.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "#### **So when might this quantity not be small?**\n",
    "\n",
    "Let us imagine that we have a training set ’è, where we have challenges x<sup>1</sup> through x<sup>m</sup>, and labels y<sup>1</sup> through y<sup>m</sup>., where x<sup>i</sup>‚àà{0,1}<sup>n</sup> and y<sup>i</sup>‚àà{0,1}. Assume all x<sup>i</sup> are distinct.\n",
    "\n",
    "A learner is given ’è, and provides the classifier that is an exact copy of the training set. In this case, the learner is memorizing the training set. No real learning has occured, and unless ’è is the entire set of all possible inputs, this is a bad classifier. \n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.3.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "Let us again consider a case with training set ’è as described before. We can build a decision tree at least of the size of ’è, that is consistent with all the points in ’è. \n",
    "\n",
    "**Then the question is: *How well does this tree generalize?* or, *What is the true error of this tree?***\n",
    "\n",
    "It will be pretty bad, qualitatively, because it is simply memorizing every entry in the training set.\n",
    "\n",
    "In our decision trees, we only considered having a low training error. To create a robust tree that can handle real data, we also need to consider, are we getting true low generalization error? We want a good combination of both things.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How can we estimate the true error of a decision tree?**\n",
    "\n",
    "A **\"hold-out\"** or **\"validation set\"** is used for this purpose.\n",
    "\n",
    "The idea is that we have ’è, the training set, and then we will have some more data that we will not let the decision tree look at, called ·ïº, our hold out.\n",
    "\n",
    "1. Use ’è to build a decision tree\n",
    "2. Estimate the tree's true error via its error on ·ïº\n",
    "\n",
    "By counting the number of mistakes that our tree makes on ·ïº, we will compute the error rate of the tree on ·ïº, and we'll use that for the estimate of the true error. As long as ·ïº is suffiently large, for any fixed tree we have built using ’è, the estimate of its true error will be very close to its error on ·ïº, which is something you can prove.\n",
    "\n",
    "It is important that you do not go back to the tree and modify it over and over to reduce error on ·ïº, because at that point you are now incorporating the validation set into the tree with ’è. This would be considered **over-fitting**.\n",
    "\n",
    "It is also important to consider that hold-out sets can be expensive, especially if you create many models and larger models.\n",
    "\n",
    "To combat this, we can use a technique called **cross-validation**, which allows you to reuse data that has been held out in a validation set. This will be covered later.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"1.3.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04bee75e-e64a-477d-aba0-b604fd693623",
   "metadata": {},
   "source": [
    "### 1.3.1 Model Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cb1ad2-f5b3-4042-8532-fd4a06686b6c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18827800-a4e9-4464-b665-46fc3c28185d",
   "metadata": {},
   "source": [
    "### 1.3.2 PAC Model of Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a556a66a-0fed-4d32-b778-5537b8d6c875",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e331c0b8-6949-4c9f-987c-ee3a1c096643",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## &emsp;&emsp;Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62afbdfa-73ca-46d7-ab00-667300bc0a8f",
   "metadata": {},
   "source": [
    "**[]()** (Local link)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
