{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6be158dc-a876-43cb-9c39-279dd11cea45",
   "metadata": {},
   "source": [
    "# PAC Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f41c711-1504-4cd0-93a3-aa8a6963a859",
   "metadata": {},
   "source": [
    "## ***Vocabulary***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36ceb54-7137-431e-8ae1-1053d0f2bcbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7dc47029-8dd1-4934-8f85-9487837b1e2c",
   "metadata": {},
   "source": [
    "# Lecture Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641eb618-a347-4dd5-bb64-a4efac8cacf4",
   "metadata": {},
   "source": [
    "## ***Infinite Function Class***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e2ec31-1e8c-44c3-bb67-c6f2b821879c",
   "metadata": {},
   "source": [
    "**PAC-learning axis-parallel rectangles**\n",
    "\n",
    "We are working in two-dimensions. Axis-parallel rectangles implies the axes are parallel to the x and y axis.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.4.1.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**What is the learning problem?** We are going to be given points, and some will be labeled positive and some will be labeled negatvie. Positive label means the point is inside c, an unknown axis-parallel rectangle. Negative labels mean the point is outside c, the unknown axis-parallel rectangle.\n",
    "\n",
    "**Goal**: given $\\epsilon$, $\\delta$ output h that is $\\epsilon$ accurate with probability $\\ge$ 1-$\\delta$.\n",
    "\n",
    "    Recall in 1.3.2 we used A which produced a consistent result (the consistency algorithm), because that depended on the size of the function class, |C|. In this case we have infinitely many axis-parallel rectangles. \n",
    "\n",
    "We will claim that the tightest fitting rectangle will solve this problem.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.4.2.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**Claim**: Tightest fitting rectangle works for this problem\\\n",
    "**Question**: How large to choose our training set (|S|) \n",
    "\n",
    "**What is a bad event in this case?** The positive points are clustered around some tiny rectangle that is very small with respect to the true rectangle. I.e., lots of probability mass exists outside of h. If the probability of landing in that probability mass is at least $\\epsilon$ then we are in trouble.\n",
    "\n",
    "How do we bound the probability this happens?\n",
    "\n",
    "Let's analyze h (the tightest fitting rectangle), and say something about h vs. all the rectangles that are large and contain h (the bad events). Since there are pretty much infinitely many bad events, then this will be hard to analyze. \n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.4.3.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "If neither B<sub>1</sub>, B<sub>2</sub>, B<sub>3</sub>, or B<sub>4</sub> occur (see image below), then h (the tightest fitting rectangle) is $\\epsilon$ accurate.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.4.4.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**We still need to figure out how large we want |S| to be.**\n",
    "\n",
    "If we choose m random samples, what is Pr[B<sub>1</sub>]?\n",
    "- $Pr[B_1] \\le (1-\\frac{\\epsilon}{4})^m$\n",
    "\n",
    "Then, by that logic:\n",
    "\n",
    "$$ Pr[B_1 \\lor B_2 \\lor B_3 \\lor B_4] \\le 4(1-\\frac{\\epsilon}{4})^m \\le \\delta$$\n",
    "\n",
    "And solving that inequality (step 3 using 1+x ≈ e<sup>x</sup>, so 1-x ≈ e<sup>-x</sup>:\n",
    "\n",
    "$$ 4(1-\\frac{\\epsilon}{4})^m \\le \\delta $$\n",
    "\n",
    "$$ (1-\\frac{\\epsilon}{4})^m \\le \\frac{\\delta}{4} $$\n",
    "\n",
    "$$ e^{-\\frac{\\epsilon m}{4}} \\le \\frac{\\delta}{4} $$ \n",
    "\n",
    "$$ -\\frac{\\epsilon m}{4} \\le log(\\frac{\\delta}{4}) $$\n",
    "\n",
    "$$ m \\ge \\frac{4*log(\\frac{\\delta}{4})}{\\epsilon} $$\n",
    "\n",
    "So, as long as we choose a number of samples that is at least $\\frac{4*log(\\frac{\\delta}{4})}{\\epsilon}$, then we know that h, the tightest fitting rectangle, will be $\\epsilon$ accurate with probability $\\ge\\;1-\\delta$.\n",
    "\n",
    "**Correction**\n",
    "\n",
    "At around 28:30 of 1.4.0, $ m \\ge \\frac{4*log(\\frac{\\delta}{4})}{\\epsilon} $ should instead be $ m \\ge \\frac{4*log(\\frac{4}{\\delta})}{\\epsilon} $. Note the inversion of the expression inside the log."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3ed5cd-43a9-4384-be36-0a59ee244e86",
   "metadata": {},
   "source": [
    "## ***Half Spaces***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fb82a9-1780-4c14-9406-a59c82ba1d90",
   "metadata": {},
   "source": [
    "Another interesting class to consider is C = {half spaces}. This is a function that takes in x and outputs sign(w*x-$\\theta$), where w $\\epsilon$ ℝ<sup>n</sup>, x $\\epsilon$ ℝ<sup>n</sup>, and $\\theta$) is a scalar (ℝ). \n",
    "\n",
    "The output f is boolean (0 or 1). The output is 0 if the result is zero or negative, and 1 if the result is positive.\n",
    "\n",
    "We can think of this result geometrically as dividing all of n-dimensional euclidean space into two half spaces.\n",
    "\n",
    "The goal is to come up with PAC learning algorithms for half spaces.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.4.5.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n",
    "\n",
    "**One approach for learning half spaces**\n",
    "\n",
    "Recall that w is unknown, $\\theta$ is unknown, and the function we are trying to learn is equal to:\n",
    "\n",
    "$$f = sign(\\sum_{i=1}^{n} w_ix_i-\\theta) $$\n",
    "\n",
    "Draws are given from D of the form (x, f(x)), where x is distributed according to D. \n",
    "\n",
    "$$ (0 1 0 1 0, pos) \\Rightarrow w_2 + w_4 \\gt \\theta $$\n",
    "\n",
    "$$ (0 1 1 0, neg) \\Rightarrow w_2 + w_3 \\le \\theta $$\n",
    "\n",
    "Each labeled example corresponds to a linear inequality. Thus, we will get a system of linear inequalities. \n",
    "\n",
    "<br>\n",
    "\n",
    "**Question:**\n",
    "- Can we find with consistent hypothesis?\n",
    "\n",
    "Let us assume that $ w_i\\;\\epsilon $ ℤ in some bounded range. Now we can apply our consistency analysis, if we can come up with a consistent hypothesis.\n",
    "\n",
    "Given a system of inequalities, we can solve for a solution for the w<sub>i</sub>'s using a general purpose tool called **linear programming**. \n",
    "\n",
    "    Linear programming algorithms can be used to solve general systems of linear inequalities. Furthermore, these algorithms that solve linear programs are known to run in polynomial time.\n",
    "\n",
    "This will be covered later in class.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "    <img src=\"images/1.4.6.png\" alt=\"Professor Notes\" />\n",
    "</center>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd7fd67-696a-40a3-9e6d-00ca9e6ddd04",
   "metadata": {},
   "source": [
    "# Personal Notes #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ce61fc-115c-4357-a766-7faec52d2284",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
