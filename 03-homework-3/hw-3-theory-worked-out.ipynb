{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e3ddd57-ddc0-4cfb-a813-8ffa0421c8ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Problem 1\n",
    "\n",
    "## **Problem Statement:** \n",
    "\n",
    "Let $A$ be an $m \\times d$ matrix, and let $X = AA^T$. Assume that $X$ has $d$ distinct, non-zero eigenvalues. Assume that $m ≫ d$. In order to find the eigendecomposition of $X$, we will need to find the eigendecomposition of an $m \\times m$ matrix. Since $m$ is much larger than $d$, this is slow. Give an algorithm for finding the eigenvectors and eigenvalues of $X$ that only requires computing the eigendecomposition of a $d \\times d$ matrix. You can use simple matrix operations and assume that you have an eigendecomposition “black box” subroutine, but avoid using the SVD as a black box.\n",
    "\n",
    "---\n",
    "\n",
    "## **Solution**\n",
    "\n",
    "### Intuition \n",
    "Since the rank of $X$ is at most $d$ (because $A$ only has $d$ columns), we only need to compute the eigendecomposition of a smaller $d \\times d$. This is because while $AA^T$ is size $m \\times m$, $A^TA$ is size $d \\times d$, and the two matrices share the same eigenvalues because they are both derived from $A$. Thus, we can use the calculated eigenvalues for $B = A^TA$ to construct the eigenvectors of $X$.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "#### **Compute B**\n",
    "Given $A$, an $m \\times d$ matrix, a $d \\times d$ matrix $B$ will be computed as:\n",
    "\n",
    "$$ B = A^TA$$\n",
    "\n",
    "#### **Compute Eigendecomposition of B**\n",
    "The eigendecomposition of $B$ will be computed as:\n",
    "\n",
    "$$B = QDQ^T $$\n",
    "\n",
    "Where $D$ is a diagonal matrix containing the eigenvalues of $B$, and $Q$ is an orthogonal matrix whose columns are the eigenvectors of $B$.\n",
    "\n",
    "#### **Derive the Eigenvectors and Eigenvalues of X from the Eigendecomposition of B**\n",
    "**1. Define an eigenvector of $B$**\n",
    "\n",
    "Let's fix $\\mathbf{v}_i$ which is an eigenvector of $B$ in $d$-dimsional space corresponding to the eigenvalue $\\lambda_i$, using the eigenvector and eigenvalue definition $B\\mathbf{v}_i =\\lambda_i \\mathbf{v}_i$:\n",
    "\n",
    "$$B\\mathbf{v}_i = A^TA\\mathbf{v}_i = \\lambda_i \\mathbf{v}_i $$\n",
    "\n",
    "**2. Define a new vector $\\mathbf{u}_i$ to map $\\mathbf{v}_i$ into higher dimensional space**\n",
    "\n",
    "Let's define a vector $\\mathbf{u}_i$ in the larger $m$-dimensional space as:\n",
    "\n",
    "$$ \\mathbf{u}_i = A\\mathbf{v}_i $$\n",
    "\n",
    "Where multuplying $\\mathbf{u}_i$ by $A$ maps the vector $\\mathbf{v}_i$ from $d$-dimensional space to the $m$-dimensional space.\n",
    "\n",
    "**3. Show that $\\mathbf{u}_i$ is an eigenvector of $X$**\n",
    "\n",
    "To prove that $\\mathbf{u}_i$ is an eigenvector of $X$, we must show:\n",
    "\n",
    "$$ X\\mathbf{u}_i = \\lambda_i\\mathbf{u}_i$$\n",
    "\n",
    "Substituting $ \\mathbf{u}_i = A\\mathbf{v}_i $ and $ X = AA^T $ into the equation:\n",
    "\n",
    "$$ AA^T(A\\mathbf{v}_i) = \\lambda_i(A\\mathbf{v}_i) $$\n",
    "\n",
    "Since $B = A^TA$:\n",
    "\n",
    "$$ A(B\\mathbf{v}_i) = \\lambda_i(A\\mathbf{v}_i) $$\n",
    "\n",
    "And since $B\\mathbf{v}_i = \\lambda_i\\mathbf{v}_i$:\n",
    "\n",
    "$$ A (\\lambda_i\\mathbf{v}_i) = \\lambda_i(A\\mathbf{v}_i) $$\n",
    "\n",
    "Which is the same as:\n",
    "\n",
    "$$ \\lambda_i (A\\mathbf{v}_i) = \\lambda_i(A\\mathbf{v}_i) $$\n",
    "\n",
    "Proving that:\n",
    "\n",
    "$$ X\\mathbf{u}_i = \\lambda_i\\mathbf{u}_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abab69f-6221-4e3a-b1a3-79afe24cdce8",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "\n",
    "## **Problem Statement:** \n",
    "\n",
    "In this problem we explore some relationships between SVD, PCA and linear regression.\n",
    "\n",
    "(a) True or false: linear regression is primarily a technique of supervised learning, i.e. where we are trying to fit a function to labeled data.\n",
    "\n",
    "(b) True or false: PCA is primarily a technique of unsupervised learning, i.e. where we are trying to find structure in unlabeled data.\n",
    "\n",
    "(c) True or false: SVD is primarily an operation on a dataset whereas PCA is primarily an operation on a matrix.\n",
    "\n",
    "(d) A common problem in linear regression is multicollinearity, where the input variables are themselves linearly dependent. For example, imagine a healthcare data set where height is measured both in inches and centimetres. This is a problem because there may now be multiple $w$ satisfying $y = w · x$. Explain how you could use a preprocessing step to solve this problem.\n",
    "\n",
    "---\n",
    "\n",
    "## **Solution**\n",
    "\n",
    "### Part A\n",
    "\n",
    "**True**.\n",
    "\n",
    "Linear regression is a supervised learning technique, since we have a training set $\\mathbf{S}$ which has data points and associated labels. The goal is to find the best fitting line to the labeled training set so we can use the model to predict on unseen data.\n",
    "\n",
    "### Part B\n",
    "\n",
    "**True**.\n",
    "\n",
    "Since PCA is a technique to find patterns and information in a matrix of unlabeled data, it is a technique of unsupervised learning. The goal is to find the directions upon which the data has the most variance (information).\n",
    "\n",
    "### Part C\n",
    "\n",
    "**False**.\n",
    "\n",
    "SVD and PCA are both matrix operations. SVD is a technique for decomposing a matrix into a simpler structure (and also performing the eigendecomposition at the same time!).\n",
    "\n",
    "### Part D\n",
    "\n",
    "To address the multicollinearity issue, PCA can be performed as a preprocessing step. The PCA will find the directions for which the data has the most variance, or provides the most information about the dataset, and reduces the redundancy of the dataset in the process. Furthermore, **the resulting principal components will be linearly independent of one another, eliminating the multicollinearity**."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
